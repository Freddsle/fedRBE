{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from classes.client import Client\n",
    "from classes.coordinator_utils import select_common_features_variables, \\\n",
    "    compute_beta, reorder_matrix, create_beta_mask\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "### Just run this, these functions are needed in various places ###\n",
    "\n",
    "### Define the client class ###\n",
    "class ClientWrapper:\n",
    "    \"\"\"\n",
    "    Holds all information necessary for the simulation run to work.\n",
    "    Defines the input data of the client, it's name and if it should be\n",
    "    considered as the coordinator\n",
    "    \"\"\"\n",
    "    def __init__(self, id: str, input_folder: str, coordinator: bool = False):\n",
    "        self.id = id\n",
    "        self.input_folder = input_folder\n",
    "        self.is_coordinator = coordinator\n",
    "        self.client_class = None\n",
    "\n",
    "def _check_consistency_clientwrappers(clientWrappers: List[ClientWrapper]) -> None:\n",
    "    \"\"\"\n",
    "    Checks for a list of clients if they were created correctly\n",
    "    Raises a ValueError in case of inconsistencies\n",
    "    Checks:\n",
    "        1. If exactly one coordinator exists\n",
    "    \"\"\"\n",
    "    coord = False\n",
    "    for clientWrapper in clientWrappers:\n",
    "        if coord and clientWrapper.is_coordinator:\n",
    "            raise ValueError(\"More than one coordinator was defined, please check \"+\\\n",
    "                            \"the code defining the clients\")\n",
    "        if clientWrapper.is_coordinator:\n",
    "            coord = True\n",
    "    if not coord:\n",
    "        raise ValueError(\"No client instance is a coordinator, please designate \"+\\\n",
    "                        \"any client as a coordinator\")\n",
    "\n",
    "def _compare_central_federated_dfs(name:str,\n",
    "                                   central_df: pd.DataFrame,\n",
    "                                   federated_df: pd.DataFrame,\n",
    "                                   intersection_features: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Compares two dataframes for equality. First checks that index and columns\n",
    "    are the same, then analyses the element wise differences.\n",
    "    See the analyse_diff_df function for more details on the difference analysis.\n",
    "    If both dataframes contain a NaN value at the same position, this is considered\n",
    "    as equal (0 as difference).\n",
    "    Args:\n",
    "        name: Name used for printing\n",
    "        central_df: The central dataframe\n",
    "        federated_df: The federated dataframe\n",
    "        intersection_features: The features that are common to both dataframes\n",
    "    \"\"\"\n",
    "    central_df = central_df.sort_index(axis=0).sort_index(axis=1)\n",
    "    federated_df = federated_df.sort_index(axis=0).sort_index(axis=1)\n",
    "    print(f\"_________________________Analysing: {name}_________________________\")\n",
    "    ### compare columns and index ###\n",
    "    failed = False\n",
    "    if not central_df.columns.equals(federated_df.columns):\n",
    "        print(f\"Columns do not match for central_df and federated_df\")\n",
    "        union_cols = central_df.columns.union(federated_df.columns)\n",
    "        intercept_cols = central_df.columns.intersection(federated_df.columns)\n",
    "        print(f\"Union-Intercept of columns: {union_cols.difference(intercept_cols)}\")\n",
    "        failed = True\n",
    "    if not central_df.index.equals(federated_df.index):\n",
    "        print(f\"Rows do not match for central_df and federated_df\")\n",
    "        union_rows = central_df.index.union(federated_df.index)\n",
    "        intercept_rows = central_df.index.intersection(federated_df.index)\n",
    "        print(f\"Union-Intercept of rows: {union_rows.difference(intercept_rows)}\")\n",
    "        failed = True\n",
    "    if failed:\n",
    "        print(f\"_________________________FAILED: {name}_________________________\")\n",
    "\n",
    "    df_diff = (central_df - federated_df).abs()\n",
    "    print(f\"Max difference: {df_diff.max().max()}\")\n",
    "    print(f\"Mean difference: {df_diff.mean().mean()}\")\n",
    "    print(f\"Max diff at position: {df_diff.idxmax().idxmax()}\")\n",
    "\n",
    "    df_diff_intersect = df_diff.loc[intersection_features]\n",
    "    print(f\"Max difference in intersect: {df_diff_intersect.max().max()}\")\n",
    "    print(f\"Mean difference in intersect: {df_diff_intersect.mean().mean()}\")\n",
    "    print(f\"Max diff at position in intersect: {df_diff_intersect.idxmax().idxmax()}\")\n",
    "\n",
    "def _concat_federated_results(clientWrappers: List[ClientWrapper],\n",
    "                              samples_in_columns=True) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Concatenates the results of the federated clients into one dataframe\n",
    "    Also checks which features are common to all clients\n",
    "    and returns them\n",
    "    Args:\n",
    "        clientWrappers: List of ClientWrapper instances, containing the data\n",
    "            in the data_corrected attribute\n",
    "        samples_in_columns: If True, the samples are in the columns, if False\n",
    "            they are in the rows. For expression files this is true.\n",
    "            This decides how to aggregate the dataframes\n",
    "    Returns:\n",
    "        merged_df: The merged dataframe containing the data of all clients\n",
    "        intersection_features: The features that are common to all clients\n",
    "    \"\"\"\n",
    "    merged_df = None\n",
    "    intersection_features = set()\n",
    "    for clientWrapper in clientWrappers:\n",
    "        # get the data in the correct format\n",
    "        if not hasattr(clientWrapper, \"data_corrected\") or \\\n",
    "            clientWrapper.data_corrected is None:\n",
    "            raise ValueError(\"No data was found in the clientWrappers\")\n",
    "        corrected_data = clientWrapper.data_corrected\n",
    "        if not samples_in_columns:\n",
    "            corrected_data = corrected_data.T\n",
    "\n",
    "        cleaned_corrected_features = set(corrected_data.dropna().index)\n",
    "        # initialize the merged_df\n",
    "        if merged_df is None:\n",
    "            merged_df = corrected_data\n",
    "            intersection_features = cleaned_corrected_features\n",
    "            continue\n",
    "\n",
    "        # merge the data\n",
    "        merged_df = pd.concat([merged_df, corrected_data], axis=1)\n",
    "        intersection_features = intersection_features.intersection(cleaned_corrected_features)\n",
    "\n",
    "    # final check\n",
    "    if merged_df is None:\n",
    "        raise ValueError(\"No data was found in the clientWrappers\")\n",
    "    # reverse the Transpose if necessary\n",
    "    if not samples_in_columns:\n",
    "        merged_df = merged_df.T\n",
    "    return merged_df, list(intersection_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This part defines the data used. A ClientWrapper class is used to      ###\n",
    "### describe all cohorts. If other data should be tested, this part should ###\n",
    "### be changed                                                             ###\n",
    "### Define the different clients ###\n",
    "    # we use a helper class for each client, see the helper function\n",
    "    # code block or the later definitions here for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Microarray Data ####################\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"microarray\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'GSE38666',  # Client 1 (Coordinator)\n",
    "#     'GSE14407',  # Client 2\n",
    "#     'GSE6008',   # Client 3\n",
    "#     'GSE40595',  # Client 4\n",
    "#     'GSE26712',  # Client 5\n",
    "#     'GSE69428',  # Client 6\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Proteomics Data ####################\n",
    "\n",
    "# First define the basefolder where all files are located\n",
    "base_dir = os.path.join(\"..\")\n",
    "# Go back to the git repos root dir\n",
    "base_dir = os.path.join(base_dir, \"evaluation_data\", \"proteomics\", \"before\")\n",
    "\n",
    "# List of cohort names\n",
    "cohort_names = [\n",
    "    'lab_A',  # Client 1 (Coordinator)\n",
    "    'lab_B',  # Client 2\n",
    "    'lab_C',  # Client 3\n",
    "    'lab_D',  # Client 4\n",
    "    'lab_E',  # Client 5\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# Microbiome Data ####################\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"microbiome\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'PRJEB27928',  # Client 1 (Coordinator)\n",
    "#     'PRJEB6070',   # Client 2\n",
    "#     'PRJNA429097', # Client 3\n",
    "#     'PRJEB10878',  # Client 4\n",
    "#     'PRJNA731589', # Client 5\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################### Simulation Data ###################\n",
    "\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# # base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"balanced\", \"before\")\n",
    "# # base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"mild_imbalanced\", \"before\")\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"strong_imbalanced\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'lab1',  # Client 1 (Coordinator)\n",
    "#     'lab2',  # Client 2\n",
    "#     'lab3',  # Client 3\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize clientWrappers list\n",
    "clientWrappers: List[ClientWrapper] = []\n",
    "\n",
    "# Iterate over cohort names and create ClientWrapper instances\n",
    "for i, cohortname in enumerate(cohort_names):\n",
    "    clientWrappers.append(ClientWrapper(\n",
    "        id=cohortname,\n",
    "        input_folder=os.path.join(base_dir, cohortname),\n",
    "        coordinator=(i == 0)  # Set the first client as coordinator\n",
    "    ))\n",
    "\n",
    "# Double check that we only have one coordinator\n",
    "_check_consistency_clientwrappers(clientWrappers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure time for all clients\n",
    "time_tracker = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "###                                  INFO                                  ###\n",
    "### The following code blocks run the simulation. They are divided into    ###\n",
    "### multiple logical blocks to ease the use                                ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: all: initial ###\n",
    "### Initial reading of the input folder\n",
    "\n",
    "send_features_variables = list()\n",
    "for clientWrapper in clientWrappers:\n",
    "    # define the client class\n",
    "    cohort_name = clientWrapper.id\n",
    "    client = Client()\n",
    "    client.config_based_init(clientname = cohort_name,\n",
    "                             input_folder = clientWrapper.input_folder,\n",
    "                             use_hashing = False)\n",
    "    clientWrapper.client_class = client\n",
    "    send_features_variables.append((cohort_name,\n",
    "                                    client.position# for mask creation - to track the cohort\n",
    "                                    list(client.hash2feature.keys()),\n",
    "                                    list(client.hash2variable.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: Coordinator: global_feature_selection ###\n",
    "### Aggregate the features and variables\n",
    "\n",
    "# obtain and safe common genes and indices of design matrix\n",
    "# wait for each client to send the list of genes they have\n",
    "# also memo the feature presence matrix and feature_to_cohorts\n",
    "\n",
    "broadcast_features_variables = tuple()\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "\n",
    "        time_tracker[\"Coordinator\"] = time.time()\n",
    "\n",
    "        global_feature_names, global_variables, feature_presence_matrix, cohorts_order = \\\n",
    "            select_common_features_variables(\n",
    "                lists_of_features_and_variables=send_features_variables,\n",
    "                min_clients=1,\n",
    "                default_order=cohort_names\n",
    "            )\n",
    "\n",
    "        # # calculate the feature_presence_matrix\n",
    "        # DEPRECATED, order is calculated already in the creation of the matrix\n",
    "        # start_time = time.time()\n",
    "        # all_client_names = [cw.id for cw in clientWrappers]\n",
    "        # feature_presence_matrix = reorder_matrix(feature_presence_matrix,\n",
    "        #                                   all_client_names,\n",
    "        #                                   cohorts_order)\n",
    "        # # memo the feature presence matrix\n",
    "        # end_time = time.time()\n",
    "        # time_tracker[\"Coordinator\"] += end_time - start_time\n",
    "\n",
    "        # memo the feature presence matrix and feature_to_cohorts\n",
    "        broadcast_features_variables = global_feature_names, global_variables, cohorts_order\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] = end_time - time_tracker[\"Coordinator\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: All: validate ###\n",
    "### Expand data to fullfill the global format. Also performs consistency checks\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "\n",
    "    time_tracker[clientWrapper.id] = time.time()\n",
    "\n",
    "    global_feauture_names_hashed, global_variables_hashed = \\\n",
    "        broadcast_features_variables\n",
    "    client = clientWrapper.client_class\n",
    "    client.validate_inputs(global_variables_hashed)\n",
    "    client.set_data(global_feauture_names_hashed)\n",
    "    # get all client names to generate design matrix\n",
    "    all_client_names = [cw.id for cw in clientWrappers]\n",
    "    err = client.create_design(all_client_names[:-1])\n",
    "    if err:\n",
    "        raise ValueError(err)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] = end_time - time_tracker[clientWrapper.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulatuion: Coordinator: create design mask based on feature presence matrix ###\n",
    "### Create the mask for the design matrix based on the feature presence matrix\n",
    "### that will be used for the beta computation\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "        start_time = time.time()\n",
    "        client = clientWrapper.client_class\n",
    "\n",
    "        n=len(client.feature_names)\n",
    "        k=client.design.shape[1]\n",
    "\n",
    "        global_mask = create_beta_mask(feature_presence_matrix, n, k)\n",
    "        # memo the global mask\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: All: prepare for compute_XtX_XtY ###\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "    client.sample_names = client.design.index.values\n",
    "\n",
    "    # Error check if the design index and the data index are the same\n",
    "    # we check by comparing the sorted indexes\n",
    "    client._check_consistency_designfile()\n",
    "\n",
    "    # Extract only relevant (the global) features and samples\n",
    "    client.data = client.data.loc[client.feature_names, client.sample_names]\n",
    "    client.n_samples = len(client.sample_names)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: All: compute_XtX_XtY ###\n",
    "### Compute XtX and XtY and share it\n",
    "send_XtX_XtY_list: List[List[np.ndarray]] = list()\n",
    "for clientWrapper in clientWrappers:\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "\n",
    "    # compute XtX and XtY\n",
    "    XtX, XtY, err = client.compute_XtX_XtY()\n",
    "    if err != None:\n",
    "        raise ValueError(err)\n",
    "\n",
    "    # send XtX and XtY\n",
    "    send_XtX_XtY_list.append([XtX, XtY])\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: Coordinator: compute_beta\n",
    "### Compute the beta values and broadcast them to the others\n",
    "broadcast_betas = None # np.ndarray of shape num_features x design_columns\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        client = clientWrapper.client_class\n",
    "        beta = compute_beta(XtX_XtY_list=send_XtX_XtY_list,\n",
    "                            n=len(client.feature_names),\n",
    "                            k=client.design.shape[1],\n",
    "                            global_mask=global_mask)\n",
    "\n",
    "        # send beta to clients so they can correct their data\n",
    "        broadcast_betas = beta\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: All: include_correction\n",
    "### Corrects the individual data\n",
    "for clientWrapper in clientWrappers:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "\n",
    "    # remove the batch effects in own data and safe the results\n",
    "    client.remove_batch_effects(beta)\n",
    "    print(f\"DEBUG: Shape of corrected data: {client.data_corrected.shape}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time\n",
    "\n",
    "    # As this is a simulation we don't save the corrected data to csv, instead\n",
    "    # we save it as a variable to the clientwrapper\n",
    "    clientWrapper.data_corrected = client.data_corrected\n",
    "    # client.data_corrected.to_csv(os.path.join(os.getcwd(), \"mnt\", \"output\", \"only_batch_corrected_data.csv\"),\n",
    "    #                                 sep=self.load(\"separator\"))\n",
    "    # client.data_corrected_and_raw.to_csv(os.path.join(os.getcwd(), \"mnt\", \"output\", \"all_data.csv\"),\n",
    "    #                              sep=self.load(\"separator\"))\n",
    "    # with open(os.path.join(os.getcwd(), \"mnt\", \"output\", \"report.txt\"), \"w\") as f:\n",
    "    #     f.write(client.report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the time tracker for the coordinator\n",
    "print(f\"Time tracker for coordinator, ms: {round(time_tracker['Coordinator']*1000, 2)}\")\n",
    "\n",
    "# print the time tracker for the clients\n",
    "for clientWrapper in clientWrappers:\n",
    "    print(f\"Time tracker for {clientWrapper.id}, ms: {round(time_tracker[clientWrapper.id]*1000, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "###                                  INFO                                  ###\n",
    "###                            SIMULATION IS DONE                          ###\n",
    "### The simulation is done. The corrected data is saved in the             ###\n",
    "### clientWrapper instances. Now we analyse the data by comparing to the   ###\n",
    "### calculated centralized corrected data.                                 ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_df, intersect_features = _concat_federated_results(clientWrappers, samples_in_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE RESULTS ###\n",
    "# Microarray data\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"microarray\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Proteomics data\n",
    "federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"proteomics\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Microbiome data\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"microbiome\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Simulation data\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"simulated\", \"balanced\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"simulated\", \"mild_imbalanced\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"simulated\", \"strong_imbalanced\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Concat the federated data and read in the centralized data ###\n",
    "# central_df_path = os.path.join(os.path.dirname(base_dir), \"after\", \"central_corrected_UNION.tsv\")\n",
    "# central_df = pd.read_csv(central_df_path, sep=\"\\t\", index_col=0)\n",
    "# _compare_central_federated_dfs(\"microarray\", central_df, federated_df, intersect_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Concat the federated data and read in the centralized data ###\n",
    "central_df_path = os.path.join(os.path.dirname(os.path.dirname(base_dir)), \"proteomics\", \"after\", \"intensities_log_Rcorrected_UNION.tsv\")\n",
    "central_df = pd.read_csv(central_df_path, sep=\"\\t\", index_col=0)\n",
    "_compare_central_federated_dfs(\"proteomic data\", central_df, federated_df, intersect_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
