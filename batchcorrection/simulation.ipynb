{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "import os\n",
    "from typing import List, Tuple, Union, Dict\n",
    "from classes.client import Client\n",
    "from classes.coordinator_utils import select_common_features_variables, \\\n",
    "    compute_beta, create_beta_mask\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "### Just run this, these functions are needed in various places ###\n",
    "\n",
    "### Define the client class ###\n",
    "class ClientWrapper:\n",
    "    \"\"\"\n",
    "    Holds all information necessary for the simulation run to work.\n",
    "    Defines the input data of the client, it's name and if it should be\n",
    "    considered as the coordinator\n",
    "    \"\"\"\n",
    "    def __init__(self, id: str, input_folder: str, coordinator: bool = False):\n",
    "        self.id = id\n",
    "        self.input_folder = input_folder\n",
    "        self.is_coordinator = coordinator\n",
    "        self.client_class: Client # initiated later\n",
    "        self.data_corrected: pd.DataFrame # initiated later\n",
    "\n",
    "def _check_consistency_clientwrappers(clientWrappers: List[ClientWrapper]) -> None:\n",
    "    \"\"\"\n",
    "    Checks for a list of clients if they were created correctly\n",
    "    Raises a ValueError in case of inconsistencies\n",
    "    Checks:\n",
    "        1. If exactly one coordinator exists\n",
    "    \"\"\"\n",
    "    coord = False\n",
    "    for clientWrapper in clientWrappers:\n",
    "        if coord and clientWrapper.is_coordinator:\n",
    "            raise ValueError(\"More than one coordinator was defined, please check \"+\\\n",
    "                            \"the code defining the clients\")\n",
    "        if clientWrapper.is_coordinator:\n",
    "            coord = True\n",
    "    if not coord:\n",
    "        raise ValueError(\"No client instance is a coordinator, please designate \"+\\\n",
    "                        \"any client as a coordinator\")\n",
    "\n",
    "def _compare_central_federated_dfs(name:str,\n",
    "                                   central_df: pd.DataFrame,\n",
    "                                   federated_df: pd.DataFrame,\n",
    "                                   intersection_features: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Compares two dataframes for equality. First checks that index and columns\n",
    "    are the same, then analyses the element wise differences.\n",
    "    See the analyse_diff_df function for more details on the difference analysis.\n",
    "    If both dataframes contain a NaN value at the same position, this is considered\n",
    "    as equal (0 as difference).\n",
    "    Args:\n",
    "        name: Name used for printing\n",
    "        central_df: The central dataframe\n",
    "        federated_df: The federated dataframe\n",
    "        intersection_features: The features that are common to both dataframes\n",
    "    \"\"\"\n",
    "    central_df = central_df.sort_index(axis=0).sort_index(axis=1)\n",
    "    federated_df = federated_df.sort_index(axis=0).sort_index(axis=1)\n",
    "    print(f\"_________________________Analysing: {name}_________________________\")\n",
    "    ### compare columns and index ###\n",
    "    failed = False\n",
    "    if not central_df.columns.equals(federated_df.columns):\n",
    "        print(f\"Columns do not match for central_df and federated_df\")\n",
    "        union_cols = central_df.columns.union(federated_df.columns)\n",
    "        intercept_cols = central_df.columns.intersection(federated_df.columns)\n",
    "        print(f\"Union-Intercept of columns: {union_cols.difference(intercept_cols)}\")\n",
    "        print(f\"Central corrected columns: {len(central_df.columns)}\")\n",
    "        print(f\"Federated corrected columns: {len(federated_df.columns)}\")\n",
    "        print(f\"Columns only corrected by central: {len(central_df.columns.difference(federated_df.columns))}\")\n",
    "        print(f\"Columns only corrected by federated: {len(federated_df.columns.difference(central_df.columns))}\")\n",
    "        failed = True\n",
    "    if not central_df.index.equals(federated_df.index):\n",
    "        print(f\"Rows do not match for central_df and federated_df\")\n",
    "        union_rows = central_df.index.union(federated_df.index)\n",
    "        intercept_rows = central_df.index.intersection(federated_df.index)\n",
    "        print(f\"Union-Intercept of rows: {union_rows.difference(intercept_rows)}\")\n",
    "        print(f\"Central corrected rows: {len(central_df.index)}\")\n",
    "        print(f\"Federated corrected rows: {len(federated_df.index)}\")\n",
    "        print(f\"Rows only corrected by central: {len(central_df.index.difference(federated_df.index))}\")\n",
    "        print(f\"Rows only corrected by federated: {len(federated_df.index.difference(central_df.index))}\")\n",
    "        failed = True\n",
    "    if failed:\n",
    "        print(f\"_________________________FAILED: {name}_________________________\")\n",
    "\n",
    "    df_diff = (central_df - federated_df).abs()\n",
    "    print(f\"Max difference: {df_diff.max().max()}\")\n",
    "    print(f\"Mean difference: {df_diff.mean().mean()}\")\n",
    "    print(f\"Max diff at position: {df_diff.idxmax().idxmax()}\")\n",
    "\n",
    "    df_diff_intersect = df_diff.loc[intersection_features]\n",
    "    print(f\"Max difference in intersect: {df_diff_intersect.max().max()}\")\n",
    "    print(f\"Mean difference in intersect: {df_diff_intersect.mean().mean()}\")\n",
    "    print(f\"Max diff at position in intersect: {df_diff_intersect.idxmax().idxmax()}\")\n",
    "\n",
    "def _concat_federated_results(clientWrappers: List[ClientWrapper],\n",
    "                              samples_in_columns=True) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Concatenates the results of the federated clients into one dataframe\n",
    "    Also checks which features are common to all clients\n",
    "    and returns them\n",
    "    Args:\n",
    "        clientWrappers: List of ClientWrapper instances, containing the data\n",
    "            in the data_corrected attribute\n",
    "        samples_in_columns: If True, the samples are in the columns, if False\n",
    "            they are in the rows. For expression files this is true.\n",
    "            This decides how to aggregate the dataframes\n",
    "    Returns:\n",
    "        merged_df: The merged dataframe containing the data of all clients\n",
    "        intersection_features: The features that are common to all clients\n",
    "    \"\"\"\n",
    "    merged_df = None\n",
    "    intersection_features = set()\n",
    "    for clientWrapper in clientWrappers:\n",
    "        # get the data in the correct format\n",
    "        if not hasattr(clientWrapper, \"data_corrected\") or \\\n",
    "            clientWrapper.data_corrected is None:\n",
    "            raise ValueError(\"No data was found in the clientWrappers\")\n",
    "        corrected_data = clientWrapper.data_corrected\n",
    "        if not samples_in_columns:\n",
    "            corrected_data = corrected_data.T\n",
    "\n",
    "        cleaned_corrected_features = set(corrected_data.dropna().index)\n",
    "        # initialize the merged_df\n",
    "        if merged_df is None:\n",
    "            merged_df = corrected_data\n",
    "            intersection_features = cleaned_corrected_features\n",
    "            continue\n",
    "\n",
    "        # merge the data\n",
    "        merged_df = pd.concat([merged_df, corrected_data], axis=1)\n",
    "        intersection_features = intersection_features.intersection(cleaned_corrected_features)\n",
    "\n",
    "    # final check\n",
    "    if merged_df is None:\n",
    "        raise ValueError(\"No data was found in the clientWrappers\")\n",
    "    # reverse the Transpose if necessary\n",
    "    if not samples_in_columns:\n",
    "        merged_df = merged_df.T\n",
    "    return merged_df, list(intersection_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This part defines the data used. A ClientWrapper class is used to      ###\n",
    "### describe all cohorts.                                                  ###\n",
    "### Comment in the wanted data or add a new block for new data. No other   ###\n",
    "### changes are necessary.                                                 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Microarray Data ####################\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# datafolder = \"microarray\"\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"microarray\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'GSE38666',  # Client 1 (Coordinator)\n",
    "#     'GSE14407',  # Client 2\n",
    "#     'GSE6008',   # Client 3\n",
    "#     'GSE40595',  # Client 4\n",
    "#     'GSE26712',  # Client 5\n",
    "#     'GSE69428',  # Client 6\n",
    "# ]\n",
    "# output_name = \"microarray\"\n",
    "# central_filename = \"central_corrected_UNION.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Proteomics Data ####################\n",
    "\n",
    "# First define the basefolder where all files are located\n",
    "base_dir = os.path.join(\"..\")\n",
    "# Go back to the git repos root dir\n",
    "datafolder = \"proteomics\"\n",
    "base_dir = os.path.join(base_dir, \"evaluation_data\", datafolder, \"before\")\n",
    "\n",
    "# List of cohort names\n",
    "cohort_names = [\n",
    "    'lab_A',  # Client 1 (Coordinator)\n",
    "    'lab_B',  # Client 2\n",
    "    'lab_C',  # Client 3\n",
    "    'lab_D',  # Client 4\n",
    "    'lab_E',  # Client 5\n",
    "]\n",
    "output_name = \"proteomics\"\n",
    "central_filename = \"intensities_log_Rcorrected_UNION.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Proteomics Multibatch Data ####################\n",
    "\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# datafolder = \"proteomics_multibatch\"\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", datafolder, \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'center1', # Client 1 (Coordinator)\n",
    "#     'center2', # Client 2\n",
    "#     'center3' # Client 3\n",
    "# ]\n",
    "# output_name = \"proteomics_multibatch\"\n",
    "# central_filename = \"intensities_log_Rcorrected_UNION.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# Microbiome Data ####################\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# datafolder = \"microbiome\"\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", datafolder, \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'PRJEB27928',  # Client 1 (Coordinator)\n",
    "#     'PRJEB6070',   # Client 2\n",
    "#     'PRJNA429097', # Client 3\n",
    "#     'PRJEB10878',  # Client 4\n",
    "#     'PRJNA731589', # Client 5\n",
    "# ]\n",
    "# output_name = \"microbiome\"\n",
    "# central_filename = \"normalized_logmin_counts_5centers_Rcorrected.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################### Simulation Data ###################\n",
    "\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "\n",
    "# # There are three different setups, activate the wanted one here accordingly, deactivate the others:\n",
    "# # BALANCED\n",
    "# datafolder = os.path.join(\"simulated\", \"balanced\")\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"balanced\", \"before\")\n",
    "# output_name = \"simulated_balanced\"\n",
    "# # # MILD IMBALANCED\n",
    "# # datafolder = os.path.join(\"simulated\", \"mild_imbalanced\")\n",
    "# # base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"mild_imbalanced\", \"before\")\n",
    "# # output_name = \"simulated_mild_imbalanced\"\n",
    "# # # STRONG IMBALANCED\n",
    "# # datafolder = os.path.join(\"simulated\", \"strong_imbalanced\")\n",
    "# # base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"strong_imbalanced\", \"before\")\n",
    "# # output_name = \"simulated_strong_imbalanced\"\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'lab1',  # Client 1 (Coordinator)\n",
    "#     'lab2',  # Client 2\n",
    "#     'lab3',  # Client 3\n",
    "# ]\n",
    "# central_filename = \"intensities_R_corrected.tsv\" # is the same for all simulated setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize clientWrappers list\n",
    "clientWrappers: List[ClientWrapper] = []\n",
    "\n",
    "# Iterate over cohort names and create ClientWrapper instances\n",
    "for i, cohortname in enumerate(cohort_names):\n",
    "    clientWrappers.append(ClientWrapper(\n",
    "        id=cohortname,\n",
    "        input_folder=os.path.join(base_dir, cohortname),\n",
    "        coordinator=(i == 0)  # Set the first client as coordinator\n",
    "    ))\n",
    "\n",
    "# Double check that we only have one coordinator\n",
    "_check_consistency_clientwrappers(clientWrappers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure time for all clients\n",
    "time_tracker = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###                                  INFO                                  ###\n",
    "### The following code blocks run the simulation. They are divided into    ###\n",
    "### multiple logical blocks to ease the use                                ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'batch_col': None, 'covariates': ['Pyr'], 'data_filename': 'intensities_log_UNION.tsv', 'design_filename': 'design.tsv', 'design_separator': '\\t', 'expression_file_flag': True, 'index_col': 'rowname', 'min_samples': 2, 'normalizationMethod': None, 'position': 0, 'separator': '\\t', 'smpc': True}}\n",
      "Opening dataset ../evaluation_data/proteomics/before/lab_A/intensities_log_UNION.tsv\n",
      "Shape of rawdata(expr_file): (2549, 24)\n",
      "I got feature P39360?: False\n",
      "finished loading data, shape of data: (2549, 24), num_features: 2549, num_samples: 24\n",
      "client has P39360?: False\n",
      "client has 3ea5bc157a12312b21d02ab33f31b3a8bb666f6bdce412a922defef32d7fb12e?: False\n",
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'batch_col': None, 'covariates': ['Pyr'], 'data_filename': 'intensities_log_UNION.tsv', 'design_filename': 'design.tsv', 'design_separator': '\\t', 'expression_file_flag': True, 'index_col': 'rowname', 'min_samples': 2, 'normalizationMethod': None, 'position': 1, 'separator': '\\t', 'smpc': True}}\n",
      "Opening dataset ../evaluation_data/proteomics/before/lab_B/intensities_log_UNION.tsv\n",
      "Shape of rawdata(expr_file): (2846, 23)\n",
      "I got feature P39360?: True\n",
      "finished loading data, shape of data: (2846, 23), num_features: 2846, num_samples: 23\n",
      "client has P39360?: True\n",
      "client has 3ea5bc157a12312b21d02ab33f31b3a8bb666f6bdce412a922defef32d7fb12e?: True\n",
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'batch_col': None, 'covariates': ['Pyr'], 'data_filename': 'intensities_log_UNION.tsv', 'design_filename': 'design.tsv', 'design_separator': '\\t', 'expression_file_flag': True, 'index_col': 'rowname', 'min_samples': 2, 'normalizationMethod': None, 'position': 2, 'separator': '\\t', 'smpc': True}}\n",
      "Opening dataset ../evaluation_data/proteomics/before/lab_C/intensities_log_UNION.tsv\n",
      "Shape of rawdata(expr_file): (2820, 23)\n",
      "I got feature P39360?: True\n",
      "finished loading data, shape of data: (2820, 23), num_features: 2820, num_samples: 23\n",
      "client has P39360?: True\n",
      "client has 3ea5bc157a12312b21d02ab33f31b3a8bb666f6bdce412a922defef32d7fb12e?: True\n",
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'batch_col': None, 'covariates': ['Pyr'], 'data_filename': 'intensities_log_UNION.tsv', 'design_filename': 'design.tsv', 'design_separator': '\\t', 'expression_file_flag': True, 'index_col': 'rowname', 'min_samples': 2, 'normalizationMethod': None, 'position': 3, 'separator': '\\t', 'smpc': True}}\n",
      "Opening dataset ../evaluation_data/proteomics/before/lab_D/intensities_log_UNION.tsv\n",
      "Shape of rawdata(expr_file): (2813, 24)\n",
      "I got feature P39360?: True\n",
      "finished loading data, shape of data: (2813, 24), num_features: 2813, num_samples: 24\n",
      "client has P39360?: True\n",
      "client has 3ea5bc157a12312b21d02ab33f31b3a8bb666f6bdce412a922defef32d7fb12e?: True\n",
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'batch_col': None, 'covariates': ['Pyr'], 'data_filename': 'intensities_log_UNION.tsv', 'design_filename': 'design.tsv', 'design_separator': '\\t', 'expression_file_flag': True, 'index_col': 'rowname', 'min_samples': 2, 'normalizationMethod': None, 'position': 4, 'separator': '\\t', 'smpc': True}}\n",
      "Opening dataset ../evaluation_data/proteomics/before/lab_E/intensities_log_UNION.tsv\n",
      "Shape of rawdata(expr_file): (2401, 24)\n",
      "I got feature P39360?: False\n",
      "finished loading data, shape of data: (2401, 24), num_features: 2401, num_samples: 24\n",
      "client has P39360?: False\n",
      "client has 3ea5bc157a12312b21d02ab33f31b3a8bb666f6bdce412a922defef32d7fb12e?: False\n",
      "INFO: Each feature must at least have 7 samples in a batch to be considered for batch effect correction. Otherwise, privacy cannot be guaranteed.\n",
      "INFO: Ignoring the following 9 features for client lab_A due to privacy reasons: ['P45468', 'P26608', 'P39297', 'P64557', 'P46859', 'P32151', 'P0A754', 'P08369', 'P0AB49']\n",
      "Client has feature 624?: [False]\n",
      "INFO: Each feature must at least have 7 samples in a batch to be considered for batch effect correction. Otherwise, privacy cannot be guaranteed.\n",
      "INFO: Ignoring the following 5 features for client lab_B due to privacy reasons: ['P77214', 'P76514', 'P22586', 'P19768', 'P64631']\n",
      "Client has feature 624?: [True]\n",
      "INFO: Each feature must at least have 7 samples in a batch to be considered for batch effect correction. Otherwise, privacy cannot be guaranteed.\n",
      "INFO: Ignoring the following 22 features for client lab_C due to privacy reasons: ['P0ABL1', 'P32670', 'P69506', 'P37340', 'P39386', 'P39405', 'P0AGI1', 'P39208', 'P33934', 'P24240', 'P0DSF4', 'P76129', 'P0ABX2', 'P37597', 'P75719;P77551', 'P07364', 'P76538', 'Q46901', 'P0AAC6', 'P76146', 'P17334', 'P75825']\n",
      "Client has feature 624?: [True]\n",
      "INFO: Each feature must at least have 7 samples in a batch to be considered for batch effect correction. Otherwise, privacy cannot be guaranteed.\n",
      "INFO: Ignoring the following 2 features for client lab_D due to privacy reasons: ['P05824', 'P46923']\n",
      "Client has feature 624?: [True]\n",
      "INFO: Each feature must at least have 7 samples in a batch to be considered for batch effect correction. Otherwise, privacy cannot be guaranteed.\n",
      "INFO: Ignoring the following 5 features for client lab_E due to privacy reasons: ['P77214', 'P56614', 'P38135', 'P31463', 'P39365']\n",
      "Client has feature 624?: [False]\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: all: initial ###\n",
    "### Initial reading of the input folder\n",
    "\n",
    "send_batch_labels_covariates = list()\n",
    "send_feature_information = list()\n",
    "for clientWrapper in clientWrappers:\n",
    "    # define the client class\n",
    "    cohort_name = clientWrapper.id\n",
    "    client = Client()\n",
    "    client.config_based_init(clientname = cohort_name,\n",
    "                             input_folder = clientWrapper.input_folder,\n",
    "                             use_hashing = True)\n",
    "    print(f\"client has P39360?: {'P39360' in client.feature2hash}\")\n",
    "    print(f\"client has 3ea5bc157a12312b21d02ab33f31b3a8bb666f6bdce412a922defef32d7fb12e?: {'3ea5bc157a12312b21d02ab33f31b3a8bb666f6bdce412a922defef32d7fb12e' in client.hash2feature}\")\n",
    "    assert isinstance(client.hash2feature, dict)\n",
    "    assert isinstance(client.hash2variable, dict)\n",
    "    clientWrapper.client_class = client\n",
    "    # send the batch labels and covariates\n",
    "    send_batch_labels_covariates.append((client.batch_labels, client.hash2variable.keys()))\n",
    "\n",
    "# receive the batch labels and covariates\n",
    "# intersect covariates, union labels\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "        global_variables_hashed = set()\n",
    "        global_batch_labels = list()\n",
    "        for labels, variables in send_batch_labels_covariates:\n",
    "            # intersect the variablesP39360\n",
    "            if len(global_variables_hashed) == 0:\n",
    "                global_variables_hashed = set(variables)\n",
    "            else:\n",
    "                global_variables_hashed = global_variables_hashed.intersection(set(variables))\n",
    "            # extend the batch_labels\n",
    "            global_batch_labels.extend(labels)\n",
    "        # ensure the batch_labels are unique\n",
    "        if len(global_batch_labels) != len(set(global_batch_labels)):\n",
    "            raise ValueError(\"Batch labels are not unique\")\n",
    "        num_batches = len(global_batch_labels)\n",
    "\n",
    "# get the relevant and privacy preserving features\n",
    "for clientWrapper in clientWrappers:\n",
    "    cohort_name = clientWrapper.id\n",
    "    client = clientWrapper.client_class\n",
    "    min_samples = max(num_batches+len(global_variables_hashed)+1, client.min_samples)\n",
    "    batch_feature_presence_info: Dict[str, List[str]] = client.get_batch_feature_presence_info(min_samples=min_samples)\n",
    "    print(f\"Client has feature 624?: {['3ea5bc157a12312b21d02ab33f31b3a8bb666f6bdce412a922defef32d7fb12e' in batch_features for batch_features in batch_feature_presence_info.values()]}\")\n",
    "    send_feature_information.append((cohort_name,\n",
    "                                client.position,\n",
    "                                client.reference_batch,\n",
    "                                batch_feature_presence_info))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Found 2702 features present in at least 3 clients\n",
      "INFO: Total number of features shared: 3047\n",
      "INFO: Using given specific client order: ['lab_A', 'lab_B', 'lab_C', 'lab_D', 'lab_E']\n",
      "INFO: Cohorts order: ['lab_A', 'lab_B', 'lab_C', 'lab_D', 'lab_E']\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: Coordinator: global_feature_selection ###\n",
    "### Aggregate the features and variables\n",
    "\n",
    "# obtain and safe common genes and indices of design matrix\n",
    "# wait for each client to send the list of genes they have\n",
    "# also memo the feature presence matrix and feature_to_cohorts\n",
    "\n",
    "broadcast_features_variables = tuple()\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "\n",
    "        time_tracker[\"Coordinator\"] = time.time()\n",
    "\n",
    "        global_feature_names, feature_presence_matrix, cohorts_order = \\\n",
    "            select_common_features_variables(\n",
    "                feature_batch_info=send_feature_information,\n",
    "                min_clients=3,\n",
    "                default_order=cohort_names\n",
    "            )\n",
    "        # memo the feature presence matrix and feature_to_cohorts\n",
    "        broadcast_features_variables = global_feature_names, cohorts_order\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] = end_time - time_tracker[\"Coordinator\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Client lab_A: Inputs validated.\n",
      "Number of features available in this client: 2549\n",
      "Number of features given globally: 2702\n",
      "Number of features only available on this client: 54\n",
      "Number of features available in other clients but not this client: 207\n",
      "Adding 207 extra global features\n",
      "INFO: Client lab_A has only one batch\n",
      "INFO: Client lab_A is not the reference batch\n",
      "INFO: Client lab_B: Inputs validated.\n",
      "Number of features available in this client: 2846\n",
      "Number of features given globally: 2702\n",
      "Number of features only available on this client: 181\n",
      "Number of features available in other clients but not this client: 37\n",
      "Adding 37 extra global features\n",
      "INFO: Client lab_B has only one batch\n",
      "INFO: Client lab_B is not the reference batch\n",
      "INFO: Client lab_C: Inputs validated.\n",
      "Number of features available in this client: 2820\n",
      "Number of features given globally: 2702\n",
      "Number of features only available on this client: 142\n",
      "Number of features available in other clients but not this client: 24\n",
      "Adding 24 extra global features\n",
      "INFO: Client lab_C has only one batch\n",
      "INFO: Client lab_C is not the reference batch\n",
      "INFO: Client lab_D: Inputs validated.\n",
      "Number of features available in this client: 2813\n",
      "Number of features given globally: 2702\n",
      "Number of features only available on this client: 127\n",
      "Number of features available in other clients but not this client: 16\n",
      "Adding 16 extra global features\n",
      "INFO: Client lab_D has only one batch\n",
      "INFO: Client lab_D is not the reference batch\n",
      "INFO: Client lab_E: Inputs validated.\n",
      "Number of features available in this client: 2401\n",
      "Number of features given globally: 2702\n",
      "Number of features only available on this client: 25\n",
      "Number of features available in other clients but not this client: 326\n",
      "Adding 326 extra global features\n",
      "INFO: Client lab_E has only one batch\n",
      "INFO: Client lab_E is the reference batch\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: All: validate ###\n",
    "### Expand data to fullfill the global format. Also performs consistency checks\n",
    "for clientWrapper in clientWrappers:\n",
    "\n",
    "    time_tracker[clientWrapper.id] = time.time()\n",
    "\n",
    "    global_feauture_names_hashed, cohorts_order = \\\n",
    "        broadcast_features_variables\n",
    "    client = clientWrapper.client_class\n",
    "    client.validate_inputs(global_variables_hashed)\n",
    "    client.set_data(global_feauture_names_hashed)\n",
    "\n",
    "    err = client.create_design(cohorts_order)\n",
    "    if err:\n",
    "        raise ValueError(err)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] = end_time - time_tracker[clientWrapper.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of global mask: (2702, 6)\n",
      "Head of mask: [[False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]]\n"
     ]
    }
   ],
   "source": [
    "### Simulatuion: Coordinator: create design mask based on feature presence matrix ###\n",
    "### Create the mask for the design matrix based on the feature presence matrix\n",
    "### that will be used for the beta computation\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "        start_time = time.time()\n",
    "        client = clientWrapper.client_class\n",
    "\n",
    "        n=len(client.feature_names)\n",
    "        k=client.design.shape[1]\n",
    "\n",
    "        global_mask = create_beta_mask(feature_presence_matrix, n, k)\n",
    "        print(f\"Shape of global mask: {global_mask.shape}\")\n",
    "        print(f\"Head of mask: {global_mask[:5]}\")\n",
    "        # memo the global mask\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: All: prepare for compute_XtX_XtY ###\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "    client.sample_names = client.design.index.values\n",
    "\n",
    "    # Error check if the design index and the data index are the same\n",
    "    # we check by comparing the sorted indexes\n",
    "    client._check_consistency_designfile()\n",
    "\n",
    "    # Extract only relevant (the global) features and samples\n",
    "    client.data = client.data.loc[client.feature_names, client.sample_names]\n",
    "    client.n_samples = len(client.sample_names)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: All: compute_XtX_XtY ###\n",
    "### Compute XtX and XtY and share it\n",
    "send_XtX_XtY_list: List[List[np.ndarray]] = list()\n",
    "for clientWrapper in clientWrappers:\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "\n",
    "    # compute XtX and XtY\n",
    "    XtX, XtY, err = client.compute_XtX_XtY()\n",
    "    if err != \"\":\n",
    "        raise ValueError(err)\n",
    "\n",
    "    # send XtX and XtY\n",
    "    send_XtX_XtY_list.append([XtX, XtY])\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "P39360\n",
      "P39360\n",
      "P39360\n",
      "None\n",
      "False\n",
      "FeaturePresenceMatrix of feature 654 (3ea5bc157a12312b21d02ab33f31b3a8bb666f6bdce412a922defef32d7fb12e):\n",
      "[0 1 1 1 0]\n",
      "INFO: Error at feature 654\n",
      "Mask: [False False  True False False  True]\n",
      "submatrix: [[35. 35. 12. 11.]\n",
      " [35. 35. 12. 11.]\n",
      " [12. 12. 12.  0.]\n",
      " [11. 11.  0. 11.]]\n",
      "full XTX: [[35. 35.  0. 12. 11. 12.]\n",
      " [35. 35.  0. 12. 11. 12.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [12. 12.  0. 12.  0.  0.]\n",
      " [11. 11.  0.  0. 11.  0.]\n",
      " [12. 12.  0.  0.  0. 12.]]\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: Coordinator: compute_beta\n",
    "### Compute the beta values and broadcast them to the others\n",
    "broadcast_betas = None # np.ndarray of shape num_features x design_columns\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    print(f\"{clientWrapper.client_class.hash2feature.get('3ea5bc157a12312b21d02ab33f31b3a8bb666f6bdce412a922defef32d7fb12e', None)}\")\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        client = clientWrapper.client_class\n",
    "        #TODO: RMV\n",
    "        print(f\"{client.feature_names[654] in client.hash2feature}\")\n",
    "        print(f\"FeaturePresenceMatrix of feature 654 ({client.feature_names[654]}):\")\n",
    "        print(f\"{feature_presence_matrix[654, :]}\")\n",
    "        beta = compute_beta(XtX_XtY_list=send_XtX_XtY_list,\n",
    "                            n=len(client.feature_names),\n",
    "                            k=client.design.shape[1],\n",
    "                            global_mask=global_mask)\n",
    "\n",
    "        # send beta to clients so they can correct their data\n",
    "        broadcast_betas = beta\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Removing the batch effects with the trained betas\n",
      "INFO: Shape of corrected data after correction: (2495, 24)\n",
      "DEBUG: Shape of corrected data: (2495, 24)\n",
      "INFO: Removing the batch effects with the trained betas\n",
      "INFO: Shape of corrected data after correction: (2665, 23)\n",
      "DEBUG: Shape of corrected data: (2665, 23)\n",
      "INFO: Removing the batch effects with the trained betas\n",
      "INFO: Shape of corrected data after correction: (2678, 23)\n",
      "DEBUG: Shape of corrected data: (2678, 23)\n",
      "INFO: Removing the batch effects with the trained betas\n",
      "INFO: Shape of corrected data after correction: (2686, 24)\n",
      "DEBUG: Shape of corrected data: (2686, 24)\n",
      "INFO: Removing the batch effects with the trained betas\n",
      "INFO: Shape of corrected data after correction: (2376, 24)\n",
      "DEBUG: Shape of corrected data: (2376, 24)\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: All: include_correction\n",
    "### Corrects the individual data\n",
    "for clientWrapper in clientWrappers:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "\n",
    "    # remove the batch effects in own data and safe the results\n",
    "    client.remove_batch_effects(beta)\n",
    "    print(f\"DEBUG: Shape of corrected data: {client.data_corrected.shape}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time\n",
    "\n",
    "    # As this is a simulation we don't save the corrected data to csv, instead\n",
    "    # we save it as a variable to the clientwrapper\n",
    "    clientWrapper.data_corrected = client.data_corrected\n",
    "    # client.data_corrected.to_csv(os.path.join(os.getcwd(), \"mnt\", \"output\", \"only_batch_corrected_data.csv\"),\n",
    "    #                                 sep=self.load(\"separator\"))\n",
    "    # client.data_corrected_and_raw.to_csv(os.path.join(os.getcwd(), \"mnt\", \"output\", \"all_data.csv\"),\n",
    "    #                              sep=self.load(\"separator\"))\n",
    "    # with open(os.path.join(os.getcwd(), \"mnt\", \"output\", \"report.txt\"), \"w\") as f:\n",
    "    #     f.write(client.report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time tracker for coordinator, ms: 73.34\n",
      "Time tracker for lab_A, ms: 135.22\n",
      "Time tracker for lab_B, ms: 82.88\n",
      "Time tracker for lab_C, ms: 148.13\n",
      "Time tracker for lab_D, ms: 143.03\n",
      "Time tracker for lab_E, ms: 92.08\n"
     ]
    }
   ],
   "source": [
    "# print the time tracker for the coordinator\n",
    "print(f\"Time tracker for coordinator, ms: {round(time_tracker['Coordinator']*1000, 2)}\")\n",
    "\n",
    "# print the time tracker for the clients\n",
    "for clientWrapper in clientWrappers:\n",
    "    print(f\"Time tracker for {clientWrapper.id}, ms: {round(time_tracker[clientWrapper.id]*1000, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###                                  INFO                                  ###\n",
    "###                            SIMULATION IS DONE                          ###\n",
    "### The simulation is done. The corrected data is saved in the             ###\n",
    "### clientWrapper instances. Now we analyse the data by comparing to the   ###\n",
    "### calculated centralized corrected data.                                 ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_df, intersect_features = _concat_federated_results(clientWrappers, samples_in_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE RESULTS ###\n",
    "#federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", datafolder, \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________Analysing: proteomics_________________________\n",
      "Rows do not match for central_df and federated_df\n",
      "Union-Intercept of rows: Index(['A0A385XJE6;P0CE49;P0CE50;P0CE51;P0CE52;P0CE53;P0CE54;P0CE55;P0CE56;P0CE57;P0CE58',\n",
      "       'A5A621', 'P00722', 'P02931;P02932;P21420', 'P02932', 'P03007',\n",
      "       'P03030', 'P05050', 'P05052', 'P05100',\n",
      "       ...\n",
      "       'Q46811', 'Q46812', 'Q46814', 'Q46865', 'Q46938', 'Q47156', 'Q47157',\n",
      "       'Q47537', 'Q47702', 'Q6BEX0'],\n",
      "      dtype='object', name='rowname', length=430)\n",
      "Central corrected rows: 2272\n",
      "Federated corrected rows: 2702\n",
      "Rows only corrected by central: 0\n",
      "Rows only corrected by federated: 430\n",
      "_________________________FAILED: proteomics_________________________\n",
      "Max difference: 1.1368683772161603e-13\n",
      "Mean difference: 3.327937958514573e-14\n",
      "Max diff at position: Clinspect_E_coli_A_S5_Slot1-7_1_8641\n",
      "Max difference in intersect: 1.1368683772161603e-13\n",
      "Mean difference in intersect: 3.338782576384476e-14\n",
      "Max diff at position in intersect: Clinspect_E_coli_A_S42_Slot1-21_1_8670\n"
     ]
    }
   ],
   "source": [
    "### Concat the federated data and read in the centralized data ###\n",
    "base_dir = os.path.join(\"..\", \"evaluation_data\")\n",
    "central_df_path = os.path.join(base_dir, datafolder, \"after\", central_filename)\n",
    "central_df = pd.read_csv(central_df_path, sep=\"\\t\", index_col=0)\n",
    "_compare_central_federated_dfs(output_name, central_df, federated_df, intersect_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
