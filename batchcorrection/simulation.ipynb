{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from classes.client import Client\n",
    "from classes.coordinator_utils import select_common_features_variables, \\\n",
    "    compute_beta, reorder_matrix, create_beta_mask\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "### Just run this, these functions are needed in various places ###\n",
    "\n",
    "### Define the client class ###\n",
    "class ClientWrapper:\n",
    "    \"\"\"\n",
    "    Holds all information necessary for the simulation run to work.\n",
    "    Defines the input data of the client, it's name and if it should be\n",
    "    considered as the coordinator\n",
    "    \"\"\"\n",
    "    def __init__(self, id: str, input_folder: str, coordinator: bool = False):\n",
    "        self.id = id\n",
    "        self.input_folder = input_folder\n",
    "        self.is_coordinator = coordinator\n",
    "        self.client_class = None\n",
    "\n",
    "def _check_consistency_clientwrappers(clientWrappers: List[ClientWrapper]) -> None:\n",
    "    \"\"\"\n",
    "    Checks for a list of clients if they were created correctly\n",
    "    Raises a ValueError in case of inconsistencies\n",
    "    Checks:\n",
    "        1. If exactly one coordinator exists\n",
    "    \"\"\"\n",
    "    coord = False\n",
    "    for clientWrapper in clientWrappers:\n",
    "        if coord and clientWrapper.is_coordinator:\n",
    "            raise ValueError(\"More than one coordinator was defined, please check \"+\\\n",
    "                            \"the code defining the clients\")\n",
    "        if clientWrapper.is_coordinator:\n",
    "            coord = True\n",
    "    if not coord:\n",
    "        raise ValueError(\"No client instance is a coordinator, please designate \"+\\\n",
    "                        \"any client as a coordinator\")\n",
    "\n",
    "def _compare_central_federated_dfs(name:str,\n",
    "                                   central_df: pd.DataFrame,\n",
    "                                   federated_df: pd.DataFrame,\n",
    "                                   intersection_features: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Compares two dataframes for equality. First checks that index and columns\n",
    "    are the same, then analyses the element wise differences.\n",
    "    See the analyse_diff_df function for more details on the difference analysis.\n",
    "    If both dataframes contain a NaN value at the same position, this is considered\n",
    "    as equal (0 as difference).\n",
    "    Args:\n",
    "        name: Name used for printing\n",
    "        central_df: The central dataframe\n",
    "        federated_df: The federated dataframe\n",
    "        intersection_features: The features that are common to both dataframes\n",
    "    \"\"\"\n",
    "    central_df = central_df.sort_index(axis=0).sort_index(axis=1)\n",
    "    federated_df = federated_df.sort_index(axis=0).sort_index(axis=1)\n",
    "    print(f\"_________________________Analysing: {name}_________________________\")\n",
    "    ### compare columns and index ###\n",
    "    failed = False\n",
    "    if not central_df.columns.equals(federated_df.columns):\n",
    "        print(f\"Columns do not match for central_df and federated_df\")\n",
    "        union_cols = central_df.columns.union(federated_df.columns)\n",
    "        intercept_cols = central_df.columns.intersection(federated_df.columns)\n",
    "        print(f\"Union-Intercept of columns: {union_cols.difference(intercept_cols)}\")\n",
    "        failed = True\n",
    "    if not central_df.index.equals(federated_df.index):\n",
    "        print(f\"Rows do not match for central_df and federated_df\")\n",
    "        union_rows = central_df.index.union(federated_df.index)\n",
    "        intercept_rows = central_df.index.intersection(federated_df.index)\n",
    "        print(f\"Union-Intercept of rows: {union_rows.difference(intercept_rows)}\")\n",
    "        failed = True\n",
    "    if failed:\n",
    "        print(f\"_________________________FAILED: {name}_________________________\")\n",
    "\n",
    "    df_diff = (central_df - federated_df).abs()\n",
    "    print(f\"Max difference: {df_diff.max().max()}\")\n",
    "    print(f\"Mean difference: {df_diff.mean().mean()}\")\n",
    "    print(f\"Max diff at position: {df_diff.idxmax().idxmax()}\")\n",
    "\n",
    "    df_diff_intersect = df_diff.loc[intersection_features]\n",
    "    print(f\"Max difference in intersect: {df_diff_intersect.max().max()}\")\n",
    "    print(f\"Mean difference in intersect: {df_diff_intersect.mean().mean()}\")\n",
    "    print(f\"Max diff at position in intersect: {df_diff_intersect.idxmax().idxmax()}\")\n",
    "\n",
    "def _concat_federated_results(clientWrappers: List[ClientWrapper],\n",
    "                              samples_in_columns=True) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Concatenates the results of the federated clients into one dataframe\n",
    "    Also checks which features are common to all clients\n",
    "    and returns them\n",
    "    Args:\n",
    "        clientWrappers: List of ClientWrapper instances, containing the data\n",
    "            in the data_corrected attribute\n",
    "        samples_in_columns: If True, the samples are in the columns, if False\n",
    "            they are in the rows. For expression files this is true.\n",
    "            This decides how to aggregate the dataframes\n",
    "    Returns:\n",
    "        merged_df: The merged dataframe containing the data of all clients\n",
    "        intersection_features: The features that are common to all clients\n",
    "    \"\"\"\n",
    "    merged_df = None\n",
    "    intersection_features = set()\n",
    "    for clientWrapper in clientWrappers:\n",
    "        # get the data in the correct format\n",
    "        if not hasattr(clientWrapper, \"data_corrected\") or \\\n",
    "            clientWrapper.data_corrected is None:\n",
    "            raise ValueError(\"No data was found in the clientWrappers\")\n",
    "        corrected_data = clientWrapper.data_corrected\n",
    "        if not samples_in_columns:\n",
    "            corrected_data = corrected_data.T\n",
    "\n",
    "        cleaned_corrected_features = set(corrected_data.dropna().index)\n",
    "        # initialize the merged_df\n",
    "        if merged_df is None:\n",
    "            merged_df = corrected_data\n",
    "            intersection_features = cleaned_corrected_features\n",
    "            continue\n",
    "\n",
    "        # merge the data\n",
    "        merged_df = pd.concat([merged_df, corrected_data], axis=1)\n",
    "        intersection_features = intersection_features.intersection(cleaned_corrected_features)\n",
    "\n",
    "    # final check\n",
    "    if merged_df is None:\n",
    "        raise ValueError(\"No data was found in the clientWrappers\")\n",
    "    # reverse the Transpose if necessary\n",
    "    if not samples_in_columns:\n",
    "        merged_df = merged_df.T\n",
    "    return merged_df, list(intersection_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This part defines the data used. A ClientWrapper class is used to      ###\n",
    "### describe all cohorts. If other data should be tested, this part should ###\n",
    "### be changed                                                             ###\n",
    "### Define the different clients ###\n",
    "    # we use a helper class for each client, see the helper function\n",
    "    # code block or the later definitions here for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Microarray Data ####################\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"microarray\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'GSE38666',  # Client 1 (Coordinator)\n",
    "#     'GSE14407',  # Client 2\n",
    "#     'GSE6008',   # Client 3\n",
    "#     'GSE40595',  # Client 4\n",
    "#     'GSE26712',  # Client 5\n",
    "#     'GSE69428',  # Client 6\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Proteomics Data ####################\n",
    "\n",
    "# First define the basefolder where all files are located\n",
    "base_dir = os.path.join(\"..\")\n",
    "# Go back to the git repos root dir\n",
    "base_dir = os.path.join(base_dir, \"evaluation_data\", \"proteomics\", \"before\")\n",
    "\n",
    "# List of cohort names\n",
    "cohort_names = [\n",
    "    'lab_A',  # Client 1 (Coordinator)\n",
    "    'lab_B',  # Client 2\n",
    "    'lab_C',  # Client 3\n",
    "    'lab_D',  # Client 4\n",
    "    'lab_E',  # Client 5\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# Microbiome Data ####################\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"microbiome\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'PRJEB27928',  # Client 1 (Coordinator)\n",
    "#     'PRJEB6070',   # Client 2\n",
    "#     'PRJNA429097', # Client 3\n",
    "#     'PRJEB10878',  # Client 4\n",
    "#     'PRJNA731589', # Client 5\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################### Simulation Data ###################\n",
    "\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# # base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"balanced\", \"before\")\n",
    "# # base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"mild_imbalanced\", \"before\")\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"strong_imbalanced\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'lab1',  # Client 1 (Coordinator)\n",
    "#     'lab2',  # Client 2\n",
    "#     'lab3',  # Client 3\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize clientWrappers list\n",
    "clientWrappers: List[ClientWrapper] = []\n",
    "\n",
    "# Iterate over cohort names and create ClientWrapper instances\n",
    "for i, cohortname in enumerate(cohort_names):\n",
    "    clientWrappers.append(ClientWrapper(\n",
    "        id=cohortname,\n",
    "        input_folder=os.path.join(base_dir, cohortname),\n",
    "        coordinator=(i == 0)  # Set the first client as coordinator\n",
    "    ))\n",
    "\n",
    "# Double check that we only have one coordinator\n",
    "_check_consistency_clientwrappers(clientWrappers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure time for all clients\n",
    "time_tracker = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "###                                  INFO                                  ###\n",
    "### The following code blocks run the simulation. They are divided into    ###\n",
    "### multiple logical blocks to ease the use                                ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'min_samples': 0, 'data_filename': 'intensities.tsv', 'separator': '\\t', 'covariates': ['A'], 'design_filename': 'design.tsv', 'index_col': 'rowname', 'expression_file_flag': True}}\n",
      "Opening dataset ../evaluation_data/simulated/strong_imbalanced/before/lab1/intensities.tsv\n",
      "Shape of rawdata(expr_file): (6000, 40)\n",
      "finished loading data, shape of data: (6000, 40), num_features: 6000, num_samples: 40\n",
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'min_samples': 0, 'data_filename': 'intensities.tsv', 'separator': '\\t', 'covariates': ['A'], 'design_filename': 'design.tsv', 'index_col': 'rowname', 'expression_file_flag': True}}\n",
      "Opening dataset ../evaluation_data/simulated/strong_imbalanced/before/lab2/intensities.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of rawdata(expr_file): (6000, 80)\n",
      "finished loading data, shape of data: (6000, 80), num_features: 6000, num_samples: 80\n",
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'min_samples': 0, 'data_filename': 'intensities.tsv', 'separator': '\\t', 'covariates': ['A'], 'design_filename': 'design.tsv', 'index_col': 'rowname', 'expression_file_flag': True}}\n",
      "Opening dataset ../evaluation_data/simulated/strong_imbalanced/before/lab3/intensities.tsv\n",
      "Shape of rawdata(expr_file): (6000, 480)\n",
      "finished loading data, shape of data: (6000, 480), num_features: 6000, num_samples: 480\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: all: initial ###\n",
    "### Initial reading of the input folder\n",
    "\n",
    "send_features_variables = list()\n",
    "for clientWrapper in clientWrappers:\n",
    "    # define the client class\n",
    "    cohort_name = clientWrapper.id\n",
    "    client = Client()\n",
    "    client.config_based_init(clientname = cohort_name,\n",
    "                             input_folder = clientWrapper.input_folder,\n",
    "                             use_hashing = False)\n",
    "    clientWrapper.client_class = client\n",
    "    send_features_variables.append((cohort_name,   # for mask creation - to track the cohort\n",
    "                                    list(client.hash2feature.keys()),\n",
    "                                    list(client.hash2variable.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: Coordinator: global_feature_selection ###\n",
    "### Aggregate the features and variables\n",
    "\n",
    "# obtain and safe common genes and indices of design matrix\n",
    "# wait for each client to send the list of genes they have\n",
    "# also memo the feature presence matrix and feature_to_cohorts\n",
    "\n",
    "broadcast_features_variables = tuple()\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "\n",
    "        time_tracker[\"Coordinator\"] = time.time()\n",
    "\n",
    "        global_feature_names, global_variables, feature_presence_matrix, cohorts_order = \\\n",
    "            select_common_features_variables(\n",
    "                lists_of_features_and_variables=send_features_variables,\n",
    "                min_clients=1      # minimum number of clients that need to have the feature\n",
    "            )\n",
    "        # memo the feature presence matrix and feature_to_cohorts\n",
    "        broadcast_features_variables = global_feature_names, global_variables\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] = end_time - time_tracker[\"Coordinator\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: Coordinator: feature presence matrix ###\n",
    "### Compute the feature presence matrix that will be used for the mask creation\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "        start_time = time.time()\n",
    "        all_client_names = [cw.id for cw in clientWrappers]\n",
    "        feature_presence_matrix = reorder_matrix(feature_presence_matrix,\n",
    "                                          all_client_names,\n",
    "                                          cohorts_order)\n",
    "        # memo the feature presence matrix\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client lab1: Inputs validated.\n",
      "feature names: 6000\n",
      "global features: 6000\n",
      "Extra local features: 0\n",
      "Extra global features: 0\n",
      "Adding 0 extra global features\n",
      "Got 6000 global features and 6000 features in the data matrix\n",
      "Before reindexing got this data: (6000, 40)\n",
      "After reindexing got this data: (6000, 40)\n",
      "design was finally created:        intercept  A  lab1  lab2\n",
      "file                           \n",
      "s.6          1.0  0     1     0\n",
      "s.29         1.0  0     1     0\n",
      "s.39         1.0  0     1     0\n",
      "s.56         1.0  0     1     0\n",
      "s.57         1.0  0     1     0\n",
      "s.58         1.0  0     1     0\n",
      "s.73         1.0  0     1     0\n",
      "s.97         1.0  0     1     0\n",
      "s.99         1.0  0     1     0\n",
      "s.107        1.0  0     1     0\n",
      "s.125        1.0  0     1     0\n",
      "s.146        1.0  0     1     0\n",
      "s.158        1.0  0     1     0\n",
      "s.167        1.0  0     1     0\n",
      "s.204        1.0  0     1     0\n",
      "s.205        1.0  0     1     0\n",
      "s.212        1.0  0     1     0\n",
      "s.223        1.0  0     1     0\n",
      "s.225        1.0  0     1     0\n",
      "s.257        1.0  0     1     0\n",
      "s.270        1.0  0     1     0\n",
      "s.281        1.0  0     1     0\n",
      "s.292        1.0  0     1     0\n",
      "s.299        1.0  0     1     0\n",
      "s.306        1.0  0     1     0\n",
      "s.316        1.0  0     1     0\n",
      "s.321        1.0  0     1     0\n",
      "s.328        1.0  0     1     0\n",
      "s.330        1.0  0     1     0\n",
      "s.344        1.0  0     1     0\n",
      "s.345        1.0  0     1     0\n",
      "s.346        1.0  0     1     0\n",
      "s.375        1.0  1     1     0\n",
      "s.378        1.0  1     1     0\n",
      "s.434        1.0  1     1     0\n",
      "s.435        1.0  1     1     0\n",
      "s.472        1.0  1     1     0\n",
      "s.484        1.0  1     1     0\n",
      "s.524        1.0  1     1     0\n",
      "s.533        1.0  1     1     0\n",
      "shape of design: (40, 4)\n",
      "Client lab2: Inputs validated.\n",
      "feature names: 6000\n",
      "global features: 6000\n",
      "Extra local features: 0\n",
      "Extra global features: 0\n",
      "Adding 0 extra global features\n",
      "Got 6000 global features and 6000 features in the data matrix\n",
      "Before reindexing got this data: (6000, 80)\n",
      "After reindexing got this data: (6000, 80)\n",
      "design was finally created:        intercept  A  lab1  lab2\n",
      "file                           \n",
      "s.13         1.0  0     0     1\n",
      "s.22         1.0  0     0     1\n",
      "s.24         1.0  0     0     1\n",
      "s.31         1.0  0     0     1\n",
      "s.36         1.0  0     0     1\n",
      "...          ... ..   ...   ...\n",
      "s.585        1.0  1     0     1\n",
      "s.586        1.0  1     0     1\n",
      "s.592        1.0  1     0     1\n",
      "s.593        1.0  1     0     1\n",
      "s.597        1.0  1     0     1\n",
      "\n",
      "[80 rows x 4 columns]\n",
      "shape of design: (80, 4)\n",
      "Client lab3: Inputs validated.\n",
      "feature names: 6000\n",
      "global features: 6000\n",
      "Extra local features: 0\n",
      "Extra global features: 0\n",
      "Adding 0 extra global features\n",
      "Got 6000 global features and 6000 features in the data matrix\n",
      "Before reindexing got this data: (6000, 480)\n",
      "After reindexing got this data: (6000, 480)\n",
      "design was finally created:        intercept  A  lab1  lab2\n",
      "file                           \n",
      "s.1          1.0  0    -1    -1\n",
      "s.2          1.0  0    -1    -1\n",
      "s.3          1.0  0    -1    -1\n",
      "s.4          1.0  0    -1    -1\n",
      "s.5          1.0  0    -1    -1\n",
      "...          ... ..   ...   ...\n",
      "s.595        1.0  1    -1    -1\n",
      "s.596        1.0  1    -1    -1\n",
      "s.598        1.0  1    -1    -1\n",
      "s.599        1.0  1    -1    -1\n",
      "s.600        1.0  1    -1    -1\n",
      "\n",
      "[480 rows x 4 columns]\n",
      "shape of design: (480, 4)\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: All: validate ###\n",
    "### Expand data to fullfill the global format. Also performs consistency checks\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "\n",
    "    time_tracker[clientWrapper.id] = time.time()\n",
    "\n",
    "    global_feauture_names_hashed, global_variables_hashed = \\\n",
    "        broadcast_features_variables\n",
    "    client = clientWrapper.client_class\n",
    "    client.validate_inputs(global_variables_hashed)\n",
    "    client.set_data(global_feauture_names_hashed)\n",
    "    # get all client names to generate design matrix\n",
    "    all_client_names = [cw.id for cw in clientWrappers]\n",
    "    err = client.create_design(all_client_names[:-1])\n",
    "    if err:\n",
    "        raise ValueError(err)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] = end_time - time_tracker[clientWrapper.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulatuion: Coordinator: create design mask based on feature presence matrix ###\n",
    "### Create the mask for the design matrix based on the feature presence matrix\n",
    "### that will be used for the beta computation\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "        start_time = time.time()\n",
    "        client = clientWrapper.client_class\n",
    "\n",
    "        n=len(client.feature_names)\n",
    "        k=client.design.shape[1]\n",
    "\n",
    "        global_mask = create_beta_mask(feature_presence_matrix, n, k)\n",
    "        # memo the global mask\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: All: prepare for compute_XtX_XtY ###\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "    client.sample_names = client.design.index.values\n",
    "\n",
    "    # Error check if the design index and the data index are the same\n",
    "    # we check by comparing the sorted indexes\n",
    "    client._check_consistency_designfile()\n",
    "\n",
    "    # Extract only relevant (the global) features and samples\n",
    "    client.data = client.data.loc[client.feature_names, client.sample_names]\n",
    "    client.n_samples = len(client.sample_names)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final vectors to be sent: XtX shape: (6000, 4, 4), XtY shape: (6000, 4)\n",
      "final vectors to be sent: XtX shape: (6000, 4, 4), XtY shape: (6000, 4)\n",
      "final vectors to be sent: XtX shape: (6000, 4, 4), XtY shape: (6000, 4)\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: All: compute_XtX_XtY ###\n",
    "### Compute XtX and XtY and share it\n",
    "send_XtX_XtY_list: List[List[np.ndarray]] = list()\n",
    "for clientWrapper in clientWrappers:\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "\n",
    "    # compute XtX and XtY\n",
    "    XtX, XtY, err = client.compute_XtX_XtY()\n",
    "    if err != None:\n",
    "        raise ValueError(err)\n",
    "\n",
    "    # send XtX and XtY\n",
    "    send_XtX_XtY_list.append([XtX, XtY])\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Shape of beta: (6000, 4)\n",
      "INFO: Number of pseudo inverses: 0\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: Coordinator: compute_beta\n",
    "### Compute the beta values and broadcast them to the others\n",
    "broadcast_betas = None # np.ndarray of shape num_features x design_columns\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        client = clientWrapper.client_class\n",
    "        beta = compute_beta(XtX_XtY_list=send_XtX_XtY_list,\n",
    "                            n=len(client.feature_names),\n",
    "                            k=client.design.shape[1],\n",
    "                            global_mask=global_mask)\n",
    "\n",
    "        # send beta to clients so they can correct their data\n",
    "        broadcast_betas = beta\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start remove_batch_effects\n",
      "Shape of data: (6000, 40)\n",
      "Shape of beta:  (6000, 4)\n",
      "Beta_reduced contains 0 Nan values\n",
      "Shape of corrected data after correction: (6000, 40)\n",
      "index is Index(['prt1', 'prt10', 'prt100', 'prt1000', 'prt1001', 'prt1002', 'prt1003',\n",
      "       'prt1004', 'prt1005', 'prt1006',\n",
      "       ...\n",
      "       'prt990', 'prt991', 'prt992', 'prt993', 'prt994', 'prt995', 'prt996',\n",
      "       'prt997', 'prt998', 'prt999'],\n",
      "      dtype='object', name='rowname', length=6000)\n",
      "Amount of index found in hash2feature: 6000/6000\n",
      "After renaming got this data_corrected:               s.6      s.29      s.39      s.56      s.57      s.58      s.73  \\\n",
      "rowname                                                                         \n",
      "prt1     2.037042 -1.561229  1.319323  0.394239 -0.981225 -1.527206 -0.715851   \n",
      "prt10    1.660499  1.627477  0.998509  1.562899  1.025412  2.403214  0.838157   \n",
      "prt100  -0.165470  5.848844  4.550318 -1.451182 -4.480440 -0.779068  4.551928   \n",
      "prt1000  0.806859  0.710102  3.522089  2.080546  2.995936  1.437654 -1.354813   \n",
      "prt1001 -0.523398  1.228246 -1.350864 -0.146651 -1.968970 -1.710569 -1.676578   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "prt995  -0.822894 -0.037995  4.975888 -5.677223 -2.782342 -0.495205  5.657534   \n",
      "prt996  -2.262095 -2.662193 -2.211050  0.040161 -0.477957 -0.930583 -0.295472   \n",
      "prt997   1.900265  1.905756 -1.562129  1.029119 -2.553230  1.262155  1.953545   \n",
      "prt998  -1.505569 -1.477133 -4.556171 -0.054963 -3.277428 -1.271976  2.291671   \n",
      "prt999   0.567051 -0.834934 -3.677595 -1.535050 -0.657638 -2.310491  1.466616   \n",
      "\n",
      "             s.97      s.99     s.107  ...     s.345     s.346     s.375  \\\n",
      "rowname                                ...                                 \n",
      "prt1    -1.387753 -0.187800 -1.816344  ... -3.913673 -0.555380 -0.823750   \n",
      "prt10    0.370823  2.911864  1.621717  ...  2.494804  1.306306  0.000508   \n",
      "prt100   2.282551  4.845844 -0.857983  ... -1.393781  0.997737  0.395847   \n",
      "prt1000  0.901615  2.420406  2.367062  ...  1.635787  2.174975 -2.992510   \n",
      "prt1001 -1.796488 -0.516919 -1.310328  ...  1.050571 -0.173115 -0.899857   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "prt995   3.149541  2.522689  6.017693  ...  1.662747  3.913801 -3.940028   \n",
      "prt996  -1.802189 -2.647606 -1.170749  ... -2.434873 -1.016446 -4.172373   \n",
      "prt997  -3.302295 -0.772760  0.828092  ... -2.301830 -2.858001 -2.179754   \n",
      "prt998   2.638569 -1.332888  2.024823  ...  1.847574 -0.954737  0.646573   \n",
      "prt999  -0.822031 -1.010160 -1.633387  ... -1.182607 -3.739682 -1.667728   \n",
      "\n",
      "             s.378     s.434     s.435     s.472     s.484     s.524     s.533  \n",
      "rowname                                                                         \n",
      "prt1     -2.373208 -0.150793 -2.368494 -1.708790 -0.557873 -0.852265 -0.920491  \n",
      "prt10    -0.359274 -0.262152 -2.257088  0.046265 -0.745755  0.730505  1.186452  \n",
      "prt100    0.209683 -2.147417 -0.714843  0.266500 -0.946225  1.303625  1.409498  \n",
      "prt1000   1.157942  1.238701  0.793711  0.889835  0.423846  0.163345  1.542941  \n",
      "prt1001  -1.482166 -0.882639 -2.076908 -0.210357 -1.255682 -0.511157 -1.897262  \n",
      "...            ...       ...       ...       ...       ...       ...       ...  \n",
      "prt995   11.645598 -1.789210 -3.772066 -2.861088  5.270774 -6.065821  8.050804  \n",
      "prt996   -3.423142 -3.404364 -1.643738 -3.174410 -1.835274 -3.147332 -2.794603  \n",
      "prt997   -3.327162 -4.034445 -2.679496 -1.837276 -2.313695  0.881292 -0.697738  \n",
      "prt998   -1.142496 -0.692862  1.164915 -0.385383  1.041251 -2.023972 -2.037925  \n",
      "prt999   -1.032432 -4.293572 -1.951860 -1.948972 -4.131187 -2.734997  0.706380  \n",
      "\n",
      "[6000 rows x 40 columns]\n",
      "remove batch final corrected data shape: (6000, 40)\n",
      "DEBUG: Shape of corrected data: (6000, 40)\n",
      "start remove_batch_effects\n",
      "Shape of data: (6000, 80)\n",
      "Shape of beta:  (6000, 4)\n",
      "Beta_reduced contains 0 Nan values\n",
      "Shape of corrected data after correction: (6000, 80)\n",
      "index is Index(['prt1', 'prt10', 'prt100', 'prt1000', 'prt1001', 'prt1002', 'prt1003',\n",
      "       'prt1004', 'prt1005', 'prt1006',\n",
      "       ...\n",
      "       'prt990', 'prt991', 'prt992', 'prt993', 'prt994', 'prt995', 'prt996',\n",
      "       'prt997', 'prt998', 'prt999'],\n",
      "      dtype='object', name='rowname', length=6000)\n",
      "Amount of index found in hash2feature: 6000/6000\n",
      "After renaming got this data_corrected:              s.13      s.22      s.24      s.31      s.36      s.44      s.50  \\\n",
      "rowname                                                                         \n",
      "prt1    -2.370353 -2.498214 -1.246818 -0.157676  0.627917  1.182538  1.393696   \n",
      "prt10    3.585292  3.340233 -1.974397  2.135711  2.022308  1.692565  0.901527   \n",
      "prt100  -0.339855  4.072639  0.720644  4.770694  3.196035 -0.285292  1.748587   \n",
      "prt1000  3.165458  1.932052  1.906530  2.126825  0.894543  0.550988  2.399251   \n",
      "prt1001 -0.819380  0.379861 -2.160498  2.149142 -1.633485 -1.142153 -1.457602   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "prt995  -0.580358  0.512768  8.868771 -0.012065 -6.932498 -0.163106  5.888419   \n",
      "prt996  -2.618794 -4.245902 -2.550957 -1.689022  0.733341 -1.166145 -1.837348   \n",
      "prt997  -2.936239 -0.387504 -0.637833  0.535589 -1.060117  0.503126 -0.705706   \n",
      "prt998   1.237082  2.399313  1.612141  1.492535  2.511354 -2.881373  0.231109   \n",
      "prt999  -1.461439 -2.731784 -3.112584 -4.919236 -0.868300 -5.164945  0.670076   \n",
      "\n",
      "             s.63      s.65      s.66  ...     s.525     s.527     s.540  \\\n",
      "rowname                                ...                                 \n",
      "prt1    -3.433526 -1.056682 -1.758029  ... -4.360851 -1.498256 -0.415813   \n",
      "prt10    3.613809  2.363908  1.752438  ...  0.532750 -0.428504 -0.352061   \n",
      "prt100   4.269619  4.373594  0.900425  ... -0.182957  1.367326 -1.389582   \n",
      "prt1000  1.313828  2.103801  2.416680  ...  0.576937 -0.274458 -0.474898   \n",
      "prt1001  2.048400 -0.266672  0.265965  ... -1.154875 -1.799074 -1.672566   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "prt995  -5.209613  4.542387  3.774631  ...  2.575203  3.611159  6.304738   \n",
      "prt996   1.154907 -2.842721 -0.414238  ... -2.791172 -2.813426 -3.505780   \n",
      "prt997  -1.876928 -2.777238  1.779265  ... -0.149821  0.939446 -1.347549   \n",
      "prt998   2.397885  0.722604 -0.448552  ... -5.093240 -1.545315 -0.025338   \n",
      "prt999   0.836615 -2.445963 -0.141124  ...  0.950569 -3.216023 -2.122787   \n",
      "\n",
      "            s.549     s.579     s.585     s.586     s.592     s.593     s.597  \n",
      "rowname                                                                        \n",
      "prt1    -2.484389 -0.855909  0.508022  1.075330 -1.157148 -2.260475 -1.976898  \n",
      "prt10    0.528796  0.148045  0.174629  0.788076  0.230014  0.670648  0.795078  \n",
      "prt100  -1.802436 -0.164231  1.572578  1.260849  0.394945 -2.634499  0.338887  \n",
      "prt1000  0.175516 -0.935350  0.830204 -0.527019  0.536615  0.764160 -1.449232  \n",
      "prt1001 -1.585360 -0.369581 -2.890917 -1.757456  0.015020 -0.716245 -1.172180  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "prt995  -2.786787  2.067668 -0.812605  6.434341 -0.209112 -4.103569  3.586159  \n",
      "prt996  -3.278650 -3.427713 -1.288379 -1.625682 -3.424902 -3.205594 -0.999818  \n",
      "prt997  -0.341703  0.974590 -1.002808 -3.797948 -0.638219  0.171664 -2.088997  \n",
      "prt998  -1.584356 -3.515958 -5.988347 -2.136972 -2.666786  0.200652  0.921195  \n",
      "prt999  -1.406333 -3.214768 -3.166110  0.549976 -3.211741 -1.593014  1.419043  \n",
      "\n",
      "[6000 rows x 80 columns]\n",
      "remove batch final corrected data shape: (6000, 80)\n",
      "DEBUG: Shape of corrected data: (6000, 80)\n",
      "start remove_batch_effects\n",
      "Shape of data: (6000, 480)\n",
      "Shape of beta:  (6000, 4)\n",
      "Beta_reduced contains 0 Nan values\n",
      "Shape of corrected data after correction: (6000, 480)\n",
      "index is Index(['prt1', 'prt10', 'prt100', 'prt1000', 'prt1001', 'prt1002', 'prt1003',\n",
      "       'prt1004', 'prt1005', 'prt1006',\n",
      "       ...\n",
      "       'prt990', 'prt991', 'prt992', 'prt993', 'prt994', 'prt995', 'prt996',\n",
      "       'prt997', 'prt998', 'prt999'],\n",
      "      dtype='object', name='rowname', length=6000)\n",
      "Amount of index found in hash2feature: 6000/6000\n",
      "After renaming got this data_corrected:               s.1        s.2       s.3       s.4       s.5       s.7  \\\n",
      "rowname                                                                \n",
      "prt1     1.193404  -3.270651 -4.132693 -2.279412 -1.309892 -0.809165   \n",
      "prt10    0.594849   1.339330  0.189997  0.972271  1.113134  1.500326   \n",
      "prt100   2.098556   2.614756  2.958763  0.824537  0.865241  1.473277   \n",
      "prt1000  2.706026   0.778454  0.374938  0.904727  2.110767  1.757552   \n",
      "prt1001  0.879890   0.994759  0.017619 -1.236911  0.794302  1.523649   \n",
      "...           ...        ...       ...       ...       ...       ...   \n",
      "prt995  -0.696113 -11.854842 -2.708095 -0.670436 -5.813215 -3.309176   \n",
      "prt996  -1.540342  -2.242159 -0.422719 -1.685618 -0.308423 -2.703444   \n",
      "prt997  -1.077263   0.732025 -0.252172 -1.262157  0.387822  0.250227   \n",
      "prt998  -1.443113   1.435363  3.797452 -1.529196 -0.750027  1.375375   \n",
      "prt999  -2.986853  -1.943217 -4.059920  0.639765 -1.466510 -1.517612   \n",
      "\n",
      "              s.8       s.9      s.10      s.11  ...     s.588     s.589  \\\n",
      "rowname                                          ...                       \n",
      "prt1    -0.591944 -0.524555  0.853318  0.739981  ... -4.402239 -2.914381   \n",
      "prt10    1.442195  2.872364 -0.720366  0.539429  ...  0.714176  0.380903   \n",
      "prt100   0.340607 -1.583446 -0.580137  1.152796  ...  4.651719 -3.443079   \n",
      "prt1000  2.797863  3.704303  2.757514  2.558316  ... -1.605816 -0.419505   \n",
      "prt1001 -0.365520  1.140192  0.374061 -0.690842  ... -1.476125 -0.599221   \n",
      "...           ...       ...       ...       ...  ...       ...       ...   \n",
      "prt995   1.625187  4.867017 -2.772566  1.594380  ...  8.159698 -7.399721   \n",
      "prt996  -2.068329 -2.387196 -1.443806 -2.690251  ... -2.913789 -2.804122   \n",
      "prt997   2.243744  0.486580  2.096646 -1.157178  ... -0.129425 -2.708824   \n",
      "prt998   2.143560  0.360398 -3.094900  2.591428  ... -4.564109  1.401964   \n",
      "prt999   2.300805 -0.854033 -1.766121 -1.578985  ...  0.660151 -1.668968   \n",
      "\n",
      "            s.590     s.591      s.594     s.595     s.596     s.598  \\\n",
      "rowname                                                                \n",
      "prt1    -2.086594 -1.812412  -2.961668 -5.435587 -3.765460  0.439489   \n",
      "prt10    0.138517  1.264358   0.290394  0.088042  2.268043  0.328311   \n",
      "prt100  -0.367637  0.880989  -0.686332  2.605366  0.836426 -2.816282   \n",
      "prt1000 -0.594865 -0.359155  -0.454379 -0.114881  2.095742  2.158694   \n",
      "prt1001 -0.200604  0.452554  -1.129499 -2.349026 -1.400667 -1.584983   \n",
      "...           ...       ...        ...       ...       ...       ...   \n",
      "prt995   2.562801  2.276041  11.516755  4.799287  5.811994  1.692956   \n",
      "prt996  -2.886279 -3.995000  -2.671610 -0.994078 -3.772174 -1.195834   \n",
      "prt997  -3.872098 -0.026946   1.244027  0.094033 -0.757637  0.666844   \n",
      "prt998   0.056810  1.641590  -2.151089 -1.523137 -2.862788 -1.487993   \n",
      "prt999  -3.290255 -4.178565  -3.431445 -1.666976 -1.324912 -1.571396   \n",
      "\n",
      "            s.599     s.600  \n",
      "rowname                      \n",
      "prt1    -3.044565 -1.810954  \n",
      "prt10    1.679138  1.311190  \n",
      "prt100   0.472706  0.658523  \n",
      "prt1000 -0.480905  0.755948  \n",
      "prt1001 -1.452486 -4.742960  \n",
      "...           ...       ...  \n",
      "prt995   6.367344 -6.055506  \n",
      "prt996  -2.709825 -2.793843  \n",
      "prt997  -1.210447  1.870135  \n",
      "prt998  -0.338523 -0.823684  \n",
      "prt999  -2.009844 -0.730283  \n",
      "\n",
      "[6000 rows x 480 columns]\n",
      "remove batch final corrected data shape: (6000, 480)\n",
      "DEBUG: Shape of corrected data: (6000, 480)\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: All: include_correction\n",
    "### Corrects the individual data\n",
    "for clientWrapper in clientWrappers:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "\n",
    "    # remove the batch effects in own data and safe the results\n",
    "    client.remove_batch_effects(beta)\n",
    "    print(f\"DEBUG: Shape of corrected data: {client.data_corrected.shape}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time\n",
    "\n",
    "    # As this is a simulation we don't save the corrected data to csv, instead\n",
    "    # we save it as a variable to the clientwrapper\n",
    "    clientWrapper.data_corrected = client.data_corrected\n",
    "    # client.data_corrected.to_csv(os.path.join(os.getcwd(), \"mnt\", \"output\", \"only_batch_corrected_data.csv\"),\n",
    "    #                                 sep=self.load(\"separator\"))\n",
    "    # client.data_corrected_and_raw.to_csv(os.path.join(os.getcwd(), \"mnt\", \"output\", \"all_data.csv\"),\n",
    "    #                              sep=self.load(\"separator\"))\n",
    "    # with open(os.path.join(os.getcwd(), \"mnt\", \"output\", \"report.txt\"), \"w\") as f:\n",
    "    #     f.write(client.report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time tracker for coordinator, ms: 323.4\n",
      "Time tracker for lab1, ms: 410.29\n",
      "Time tracker for lab2, ms: 364.62\n",
      "Time tracker for lab3, ms: 639.19\n"
     ]
    }
   ],
   "source": [
    "# print the time tracker for the coordinator\n",
    "print(f\"Time tracker for coordinator, ms: {round(time_tracker['Coordinator']*1000, 2)}\")\n",
    "\n",
    "# print the time tracker for the clients\n",
    "for clientWrapper in clientWrappers:\n",
    "    print(f\"Time tracker for {clientWrapper.id}, ms: {round(time_tracker[clientWrapper.id]*1000, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "###                                  INFO                                  ###\n",
    "###                            SIMULATION IS DONE                          ###\n",
    "### The simulation is done. The corrected data is saved in the             ###\n",
    "### clientWrapper instances. Now we analyse the data by comparing to the   ###\n",
    "### calculated centralized corrected data.                                 ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_df, intersect_features = _concat_federated_results(clientWrappers, samples_in_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE RESULTS ###\n",
    "# Microarray data\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"microarray\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Proteomics data\n",
    "federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"proteomics\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Microbiome data\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"microbiome\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Simulation data\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"simulated\", \"balanced\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"simulated\", \"mild_imbalanced\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"simulated\", \"strong_imbalanced\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________Analysing: microarray_________________________\n",
      "Max difference: 8.171241461241152e-14\n",
      "Mean difference: 4.357000735613189e-15\n",
      "Max diff at position: GSM1701030\n",
      "Max difference in intersect: 8.171241461241152e-14\n",
      "Mean difference in intersect: 4.515621419262236e-15\n",
      "Max diff at position in intersect: GSM1701030\n"
     ]
    }
   ],
   "source": [
    "# ### Concat the federated data and read in the centralized data ###\n",
    "# central_df_path = os.path.join(os.path.dirname(base_dir), \"after\", \"central_corrected_UNION.tsv\")\n",
    "# central_df = pd.read_csv(central_df_path, sep=\"\\t\", index_col=0)\n",
    "# _compare_central_federated_dfs(\"microarray\", central_df, federated_df, intersect_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________Analysing: proteomic data_________________________\n",
      "Max difference: 1.9184653865522705e-13\n",
      "Mean difference: 3.1758692034102934e-14\n",
      "Max diff at position: Clinspect_E_coli_A_S42_Slot1-21_1_8670\n",
      "Max difference in intersect: 1.1368683772161603e-13\n",
      "Mean difference in intersect: 3.338152465379177e-14\n",
      "Max diff at position in intersect: Clinspect_E_coli_A_S42_Slot1-21_1_8670\n"
     ]
    }
   ],
   "source": [
    "### Concat the federated data and read in the centralized data ###\n",
    "central_df_path = os.path.join(os.path.dirname(os.path.dirname(base_dir)), \"proteomics\", \"after\", \"intensities_log_Rcorrected_UNION.tsv\")\n",
    "central_df = pd.read_csv(central_df_path, sep=\"\\t\", index_col=0)\n",
    "_compare_central_federated_dfs(\"proteomic data\", central_df, federated_df, intersect_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
