{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from classes.client import Client\n",
    "from classes.coordinator_utils import select_common_features_variables, \\\n",
    "    compute_beta, reorder_matrix, create_beta_mask\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "### Just run this, these functions are needed in various places ###\n",
    "\n",
    "### Define the client class ###\n",
    "class ClientWrapper:\n",
    "    \"\"\"\n",
    "    Holds all information necessary for the simulation run to work.\n",
    "    Defines the input data of the client, it's name and if it should be\n",
    "    considered as the coordinator\n",
    "    \"\"\"\n",
    "    def __init__(self, id: str, input_folder: str, coordinator: bool = False):\n",
    "        self.id = id\n",
    "        self.input_folder = input_folder\n",
    "        self.is_coordinator = coordinator\n",
    "        self.client_class = None\n",
    "\n",
    "def _check_consistency_clientwrappers(clientWrappers: List[ClientWrapper]) -> None:\n",
    "    \"\"\"\n",
    "    Checks for a list of clients if they were created correctly\n",
    "    Raises a ValueError in case of inconsistencies\n",
    "    Checks:\n",
    "        1. If exactly one coordinator exists\n",
    "    \"\"\"\n",
    "    coord = False\n",
    "    for clientWrapper in clientWrappers:\n",
    "        if coord and clientWrapper.is_coordinator:\n",
    "            raise ValueError(\"More than one coordinator was defined, please check \"+\\\n",
    "                            \"the code defining the clients\")\n",
    "        if clientWrapper.is_coordinator:\n",
    "            coord = True\n",
    "    if not coord:\n",
    "        raise ValueError(\"No client instance is a coordinator, please designate \"+\\\n",
    "                        \"any client as a coordinator\")\n",
    "    \n",
    "def _compare_central_federated_dfs(name:str,\n",
    "                                   central_df: pd.DataFrame,\n",
    "                                   federated_df: pd.DataFrame,\n",
    "                                   intersection_features: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Compares two dataframes for equality. First checks that index and columns\n",
    "    are the same, then analyses the element wise differences.\n",
    "    See the analyse_diff_df function for more details on the difference analysis.\n",
    "    If both dataframes contain a NaN value at the same position, this is considered\n",
    "    as equal (0 as difference).\n",
    "    Args:\n",
    "        name: Name used for printing\n",
    "        central_df: The central dataframe\n",
    "        federated_df: The federated dataframe\n",
    "        intersection_features: The features that are common to both dataframes\n",
    "    \"\"\"\n",
    "    central_df = central_df.sort_index(axis=0).sort_index(axis=1)\n",
    "    federated_df = federated_df.sort_index(axis=0).sort_index(axis=1)\n",
    "    print(f\"_________________________Analysing: {name}_________________________\")\n",
    "    ### compare columns and index ###\n",
    "    failed = False\n",
    "    if not central_df.columns.equals(federated_df.columns):\n",
    "        print(f\"Columns do not match for central_df and federated_df\")\n",
    "        union_cols = central_df.columns.union(federated_df.columns)\n",
    "        intercept_cols = central_df.columns.intersection(federated_df.columns)\n",
    "        print(f\"Union-Intercept of columns: {union_cols.difference(intercept_cols)}\")\n",
    "        failed = True\n",
    "    if not central_df.index.equals(federated_df.index):\n",
    "        print(f\"Rows do not match for central_df and federated_df\")\n",
    "        union_rows = central_df.index.union(federated_df.index)\n",
    "        intercept_rows = central_df.index.intersection(federated_df.index)\n",
    "        print(f\"Union-Intercept of rows: {union_rows.difference(intercept_rows)}\")\n",
    "        failed = True\n",
    "    if failed:\n",
    "        print(f\"_________________________FAILED: {name}_________________________\")\n",
    "\n",
    "    df_diff = (central_df - federated_df).abs()\n",
    "    print(f\"Max difference: {df_diff.max().max()}\")\n",
    "    print(f\"Mean difference: {df_diff.mean().mean()}\")\n",
    "    print(f\"Max diff at position: {df_diff.idxmax().idxmax()}\")\n",
    "\n",
    "    df_diff_intersect = df_diff.loc[intersection_features]\n",
    "    print(f\"Max difference in intersect: {df_diff_intersect.max().max()}\")\n",
    "    print(f\"Mean difference in intersect: {df_diff_intersect.mean().mean()}\")\n",
    "    print(f\"Max diff at position in intersect: {df_diff_intersect.idxmax().idxmax()}\")\n",
    "\n",
    "def _concat_federated_results(clientWrappers: List[ClientWrapper],\n",
    "                              samples_in_columns=True) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Concatenates the results of the federated clients into one dataframe\n",
    "    Also checks which features are common to all clients\n",
    "    and returns them\n",
    "    Args:\n",
    "        clientWrappers: List of ClientWrapper instances, containing the data\n",
    "            in the data_corrected attribute\n",
    "        samples_in_columns: If True, the samples are in the columns, if False\n",
    "            they are in the rows. For expression files this is true.\n",
    "            This decides how to aggregate the dataframes\n",
    "    Returns:\n",
    "        merged_df: The merged dataframe containing the data of all clients\n",
    "        intersection_features: The features that are common to all clients\n",
    "    \"\"\"\n",
    "    merged_df = None\n",
    "    intersection_features = set()\n",
    "    for clientWrapper in clientWrappers:\n",
    "        # get the data in the correct format\n",
    "        if not hasattr(clientWrapper, \"data_corrected\") or \\\n",
    "            clientWrapper.data_corrected is None:\n",
    "            raise ValueError(\"No data was found in the clientWrappers\")\n",
    "        corrected_data = clientWrapper.data_corrected\n",
    "        if not samples_in_columns:\n",
    "            corrected_data = corrected_data.T\n",
    "\n",
    "        cleaned_corrected_features = set(corrected_data.dropna().index)\n",
    "        # initialize the merged_df\n",
    "        if merged_df is None:\n",
    "            merged_df = corrected_data\n",
    "            intersection_features = cleaned_corrected_features\n",
    "            continue\n",
    "\n",
    "        # merge the data\n",
    "        merged_df = pd.concat([merged_df, corrected_data], axis=1)\n",
    "        intersection_features = intersection_features.intersection(cleaned_corrected_features)\n",
    "\n",
    "    # final check\n",
    "    if merged_df is None:\n",
    "        raise ValueError(\"No data was found in the clientWrappers\")\n",
    "    # reverse the Transpose if necessary\n",
    "    if not samples_in_columns:\n",
    "        merged_df = merged_df.T\n",
    "    return merged_df, list(intersection_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This part defines the data used. A ClientWrapper class is used to      ###\n",
    "### describe all cohorts. If other data should be tested, this part should ###\n",
    "### be changed                                                             ###\n",
    "### Define the different clients ###\n",
    "    # we use a helper class for each client, see the helper function\n",
    "    # code block or the later definitions here for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Microarray Data ####################\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"microarray\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'GSE38666',  # Client 1 (Coordinator)\n",
    "#     'GSE14407',  # Client 2\n",
    "#     'GSE6008',   # Client 3\n",
    "#     'GSE40595',  # Client 4\n",
    "#     'GSE26712',  # Client 5\n",
    "#     'GSE69428',  # Client 6\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Proteomics Data ####################\n",
    "\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"proteomics\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'lab_A',  # Client 1 (Coordinator)\n",
    "#     'lab_B',  # Client 2\n",
    "#     'lab_C',  # Client 3\n",
    "#     'lab_D',  # Client 4\n",
    "#     'lab_E',  # Client 5\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# Microbiome Data ####################\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"microbiome\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'PRJEB27928',  # Client 1 (Coordinator)\n",
    "#     'PRJEB6070',   # Client 2\n",
    "#     'PRJNA429097', # Client 3\n",
    "#     'PRJEB10878',  # Client 4\n",
    "#     'PRJNA731589', # Client 5\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Simulation Data ###################\n",
    "\n",
    "# First define the basefolder where all files are located\n",
    "base_dir = os.path.join(\"..\")\n",
    "# Go back to the git repos root dir\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"balanced\", \"before\")\n",
    "base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"mild_imbalanced\", \"before\")\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"strong_imbalanced\", \"before\")\n",
    "\n",
    "# List of cohort names\n",
    "cohort_names = [\n",
    "    'lab1',  # Client 1 (Coordinator)\n",
    "    'lab2',  # Client 2\n",
    "    'lab3',  # Client 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize clientWrappers list\n",
    "clientWrappers: List[ClientWrapper] = []\n",
    "\n",
    "# Iterate over cohort names and create ClientWrapper instances\n",
    "for i, cohortname in enumerate(cohort_names):\n",
    "    clientWrappers.append(ClientWrapper(\n",
    "        id=cohortname,\n",
    "        input_folder=os.path.join(base_dir, cohortname),\n",
    "        coordinator=(i == 0)  # Set the first client as coordinator\n",
    "    ))\n",
    "\n",
    "# Double check that we only have one coordinator\n",
    "_check_consistency_clientwrappers(clientWrappers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure time for all clients\n",
    "time_tracker = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "###                                  INFO                                  ###\n",
    "### The following code blocks run the simulation. They are divided into    ###\n",
    "### multiple logical blocks to ease the use                                ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'min_samples': 2, 'data_filename': 'intensities.tsv', 'separator': '\\t', 'covariates': ['A'], 'design_filename': 'design.tsv', 'index_col': 'rowname', 'expression_file_flag': True}}\n",
      "Opening dataset ../evaluation_data/simulated/balanced/before/lab1/intensities.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of rawdata(expr_file): (6000, 200)\n",
      "finished loading data, shape of data: (6000, 200), num_features: 6000, num_samples: 200\n",
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'min_samples': 2, 'data_filename': 'intensities.tsv', 'separator': '\\t', 'covariates': ['A'], 'design_filename': 'design.tsv', 'index_col': 'rowname', 'expression_file_flag': True}}\n",
      "Opening dataset ../evaluation_data/simulated/balanced/before/lab2/intensities.tsv\n",
      "Shape of rawdata(expr_file): (6000, 200)\n",
      "finished loading data, shape of data: (6000, 200), num_features: 6000, num_samples: 200\n",
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'min_samples': 2, 'data_filename': 'intensities.tsv', 'separator': '\\t', 'covariates': ['A'], 'design_filename': 'design.tsv', 'index_col': 'rowname', 'expression_file_flag': True}}\n",
      "Opening dataset ../evaluation_data/simulated/balanced/before/lab3/intensities.tsv\n",
      "Shape of rawdata(expr_file): (6000, 200)\n",
      "finished loading data, shape of data: (6000, 200), num_features: 6000, num_samples: 200\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: all: initial ###\n",
    "### Initial reading of the input folder\n",
    "\n",
    "send_features_variables = list()\n",
    "for clientWrapper in clientWrappers:\n",
    "    # define the client class\n",
    "    cohort_name = clientWrapper.id\n",
    "    client = Client()\n",
    "    client.config_based_init(clientname = cohort_name,\n",
    "                             input_folder = clientWrapper.input_folder,\n",
    "                             use_hashing = False)\n",
    "    clientWrapper.client_class = client\n",
    "    send_features_variables.append((cohort_name,   # for mask creation - to track the cohort\n",
    "                                    list(client.hash2feature.keys()), \n",
    "                                    list(client.hash2variable.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: Coordinator: global_feature_selection ###\n",
    "### Aggregate the features and variables\n",
    "\n",
    "# obtain and safe common genes and indices of design matrix\n",
    "# wait for each client to send the list of genes they have\n",
    "# also memo the feature presence matrix and feature_to_cohorts\n",
    "\n",
    "broadcast_features_variables = tuple()\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "        \n",
    "        time_tracker[\"Coordinator\"] = time.time()\n",
    "\n",
    "        global_feature_names, global_variables, feature_presence_matrix, cohorts_order = \\\n",
    "            select_common_features_variables(\n",
    "                lists_of_features_and_variables=send_features_variables,\n",
    "                min_clients=1      # minimum number of clients that need to have the feature\n",
    "            )\n",
    "        # memo the feature presence matrix and feature_to_cohorts\n",
    "        broadcast_features_variables = global_feature_names, global_variables\n",
    "        \n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] = end_time - time_tracker[\"Coordinator\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: Coordinator: feature presence matrix ###\n",
    "### Compute the feature presence matrix that will be used for the mask creation\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "        start_time = time.time()\n",
    "        all_client_names = [cw.id for cw in clientWrappers]\n",
    "        feature_presence_matrix = reorder_matrix(feature_presence_matrix, \n",
    "                                          all_client_names, \n",
    "                                          cohorts_order)\n",
    "        # memo the feature presence matrix\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client lab1: Inputs validated.\n",
      "feature names: 6000\n",
      "global features: 6000\n",
      "Extra local features: 0\n",
      "Extra global features: 0\n",
      "Adding 0 extra global features\n",
      "Got 6000 global features and 6000 features in the data matrix\n",
      "Before reindexing got this data: (6000, 200)\n",
      "After reindexing got this data: (6000, 200)\n",
      "design was finally created:        intercept  A  lab1  lab2\n",
      "file                           \n",
      "s.3          1.0  0     1     0\n",
      "s.4          1.0  0     1     0\n",
      "s.8          1.0  0     1     0\n",
      "s.9          1.0  0     1     0\n",
      "s.12         1.0  0     1     0\n",
      "...          ... ..   ...   ...\n",
      "s.592        1.0  1     1     0\n",
      "s.594        1.0  1     1     0\n",
      "s.595        1.0  1     1     0\n",
      "s.596        1.0  1     1     0\n",
      "s.600        1.0  1     1     0\n",
      "\n",
      "[200 rows x 4 columns]\n",
      "shape of design: (200, 4)\n",
      "Client lab2: Inputs validated.\n",
      "feature names: 6000\n",
      "global features: 6000\n",
      "Extra local features: 0\n",
      "Extra global features: 0\n",
      "Adding 0 extra global features\n",
      "Got 6000 global features and 6000 features in the data matrix\n",
      "Before reindexing got this data: (6000, 200)\n",
      "After reindexing got this data: (6000, 200)\n",
      "design was finally created:        intercept  A  lab1  lab2\n",
      "file                           \n",
      "s.5          1.0  0     0     1\n",
      "s.11         1.0  0     0     1\n",
      "s.15         1.0  0     0     1\n",
      "s.16         1.0  0     0     1\n",
      "s.21         1.0  0     0     1\n",
      "...          ... ..   ...   ...\n",
      "s.586        1.0  1     0     1\n",
      "s.588        1.0  1     0     1\n",
      "s.597        1.0  1     0     1\n",
      "s.598        1.0  1     0     1\n",
      "s.599        1.0  1     0     1\n",
      "\n",
      "[200 rows x 4 columns]\n",
      "shape of design: (200, 4)\n",
      "Client lab3: Inputs validated.\n",
      "feature names: 6000\n",
      "global features: 6000\n",
      "Extra local features: 0\n",
      "Extra global features: 0\n",
      "Adding 0 extra global features\n",
      "Got 6000 global features and 6000 features in the data matrix\n",
      "Before reindexing got this data: (6000, 200)\n",
      "After reindexing got this data: (6000, 200)\n",
      "design was finally created:        intercept  A  lab1  lab2\n",
      "file                           \n",
      "s.1          1.0  0    -1    -1\n",
      "s.2          1.0  0    -1    -1\n",
      "s.6          1.0  0    -1    -1\n",
      "s.7          1.0  0    -1    -1\n",
      "s.10         1.0  0    -1    -1\n",
      "...          ... ..   ...   ...\n",
      "s.576        1.0  1    -1    -1\n",
      "s.582        1.0  1    -1    -1\n",
      "s.585        1.0  1    -1    -1\n",
      "s.590        1.0  1    -1    -1\n",
      "s.593        1.0  1    -1    -1\n",
      "\n",
      "[200 rows x 4 columns]\n",
      "shape of design: (200, 4)\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: All: validate ###\n",
    "### Expand data to fullfill the global format. Also performs consistency checks\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "\n",
    "    time_tracker[clientWrapper.id] = time.time()\n",
    "\n",
    "    global_feauture_names_hashed, global_variables_hashed = \\\n",
    "        broadcast_features_variables\n",
    "    client = clientWrapper.client_class\n",
    "    client.validate_inputs(global_variables_hashed)\n",
    "    client.set_data(global_feauture_names_hashed)\n",
    "    # get all client names to generate design matrix\n",
    "    all_client_names = [cw.id for cw in clientWrappers]\n",
    "    err = client.create_design(all_client_names[:-1])\n",
    "    if err:\n",
    "        raise ValueError(err)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] = end_time - time_tracker[clientWrapper.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulatuion: Coordinator: create design mask based on feature presence matrix ###\n",
    "### Create the mask for the design matrix based on the feature presence matrix\n",
    "### that will be used for the beta computation\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "        start_time = time.time()\n",
    "        client = clientWrapper.client_class\n",
    "\n",
    "        n=len(client.feature_names)\n",
    "        k=client.design.shape[1]\n",
    "\n",
    "        global_mask = create_beta_mask(feature_presence_matrix, n, k)\n",
    "        # memo the global mask\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: All: prepare for compute_XtX_XtY ###\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    client = clientWrapper.client_class\n",
    "    client.sample_names = client.design.index.values\n",
    "\n",
    "    # Error check if the design index and the data index are the same\n",
    "    # we check by comparing the sorted indexes\n",
    "    client._check_consistency_designfile()\n",
    "\n",
    "    # Extract only relevant (the global) features and samples\n",
    "    client.data = client.data.loc[client.feature_names, client.sample_names]\n",
    "    client.n_samples = len(client.sample_names)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final vectors to be sent: XtX shape: (6000, 4, 4), XtY shape: (6000, 4)\n",
      "final vectors to be sent: XtX shape: (6000, 4, 4), XtY shape: (6000, 4)\n",
      "final vectors to be sent: XtX shape: (6000, 4, 4), XtY shape: (6000, 4)\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: All: compute_XtX_XtY ###\n",
    "### Compute XtX and XtY and share it\n",
    "send_XtX_XtY_list: List[List[np.ndarray]] = list()\n",
    "for clientWrapper in clientWrappers:\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "\n",
    "    # compute XtX and XtY\n",
    "    XtX, XtY, err = client.compute_XtX_XtY()\n",
    "    if err != None:\n",
    "        raise ValueError(err)\n",
    "\n",
    "    # send XtX and XtY\n",
    "    send_XtX_XtY_list.append([XtX, XtY])\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Shape of beta: (6000, 4)\n",
      "INFO: Number of pseudo inverses: 0\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: Coordinator: compute_beta\n",
    "### Compute the beta values and broadcast them to the others\n",
    "broadcast_betas = None # np.ndarray of shape num_features x design_columns\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "\n",
    "        start_time = time.time()    \n",
    "\n",
    "        client = clientWrapper.client_class\n",
    "        beta = compute_beta(XtX_XtY_list=send_XtX_XtY_list,\n",
    "                            n=len(client.feature_names),\n",
    "                            k=client.design.shape[1],\n",
    "                            global_mask=global_mask)\n",
    "\n",
    "        # send beta to clients so they can correct their data\n",
    "        broadcast_betas = beta\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start remove_batch_effects\n",
      "Shape of data: (6000, 200)\n",
      "Shape of beta:  (6000, 4)\n",
      "Beta_reduced contains 0 Nan values\n",
      "Shape of corrected data after correction: (6000, 200)\n",
      "index is Index(['prt1', 'prt10', 'prt100', 'prt1000', 'prt1001', 'prt1002', 'prt1003',\n",
      "       'prt1004', 'prt1005', 'prt1006',\n",
      "       ...\n",
      "       'prt990', 'prt991', 'prt992', 'prt993', 'prt994', 'prt995', 'prt996',\n",
      "       'prt997', 'prt998', 'prt999'],\n",
      "      dtype='object', name='rowname', length=6000)\n",
      "Amount of index found in hash2feature: 6000/6000\n",
      "After renaming got this data_corrected:               s.3       s.4       s.8       s.9      s.12      s.17      s.19  \\\n",
      "rowname                                                                         \n",
      "prt1     0.706849  0.737818 -0.198409  0.765899  2.180895 -2.273794  0.619105   \n",
      "prt10    0.766437  0.121123 -1.705206 -2.882978 -0.741112 -1.893073 -0.027280   \n",
      "prt100  -0.725172 -5.856573  3.196547  2.593832  3.815922 -0.785160 -0.424008   \n",
      "prt1000 -5.832140  0.842895 -1.720141  2.737808 -5.253596 -4.571885 -0.323932   \n",
      "prt1001  0.227673  0.303903  0.159185 -0.561641  0.259173  1.074807 -0.095848   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "prt995   1.527560  0.902288 -0.736768 -1.730726  2.444892 -0.024123 -1.953205   \n",
      "prt996   2.317245 -1.161673 -1.182913  1.916955  1.591177  1.627605 -6.028253   \n",
      "prt997  -2.423300  0.712655  0.098322  0.174158 -0.448322 -1.052488  0.020937   \n",
      "prt998   2.010362 -0.269314  1.024408  3.029198  1.892686  2.128220  1.625253   \n",
      "prt999   0.263076 -0.824198  3.332623  1.440352  2.574061  1.810687 -2.822965   \n",
      "\n",
      "             s.20      s.23      s.24  ...     s.583     s.584     s.587  \\\n",
      "rowname                                ...                                 \n",
      "prt1     1.344603  1.830675 -0.859646  ... -0.342351 -0.765938 -0.053268   \n",
      "prt10   -1.532511 -1.118383 -0.116767  ... -0.895445 -1.524859 -2.545834   \n",
      "prt100   0.255805 -0.175555 -2.279216  ... -4.671529 -2.404928 -2.906583   \n",
      "prt1000  0.050677  0.989602  2.996793  ... -7.085493 -1.021035  5.632385   \n",
      "prt1001 -0.329117  0.756402  0.553532  ... -1.449943 -1.610152 -1.188030   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "prt995   1.545341  1.239902 -1.829185  ... -2.105891 -3.145692  0.215296   \n",
      "prt996   0.878201  1.405613 -1.076891  ...  0.537543 -2.959912 -0.982514   \n",
      "prt997   2.383920  0.959696  0.858408  ... -0.898621 -1.225967 -1.574639   \n",
      "prt998  -0.455729  3.921282  1.035927  ...  0.965560  0.290978  0.652723   \n",
      "prt999   0.774548  2.951108  2.324370  ... -2.931331 -5.440786 -0.806187   \n",
      "\n",
      "            s.589     s.591     s.592     s.594     s.595     s.596     s.600  \n",
      "rowname                                                                        \n",
      "prt1    -0.303033 -1.946506 -0.757450 -2.301882 -0.640235 -1.185445 -2.037918  \n",
      "prt10   -0.727331 -1.824680  0.455801 -1.746503 -1.669579 -3.126747 -2.391785  \n",
      "prt100  -1.685840  3.303804  0.016455 -0.629640 -3.196845 -0.443406 -0.167306  \n",
      "prt1000  2.577494 -3.160239 -5.398186 -4.593087 -5.991287  1.076994 -2.603688  \n",
      "prt1001 -1.068129 -1.220514 -1.257947 -0.210860 -1.105836 -1.330531 -1.780578  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "prt995   0.911775 -1.978020  0.057393  0.377531  2.443439 -1.381984 -2.696569  \n",
      "prt996  -2.415378 -2.472327 -0.746631 -2.248486 -0.772429 -1.718635 -0.772041  \n",
      "prt997  -2.036108 -1.172932  0.485098 -0.558391 -1.748251 -3.660733 -1.868018  \n",
      "prt998   0.801991 -1.646708  2.157331 -0.283609 -0.874499  0.264244 -1.811532  \n",
      "prt999  -2.792186 -0.052112 -0.229669 -1.449806 -0.567427 -0.130422 -0.262910  \n",
      "\n",
      "[6000 rows x 200 columns]\n",
      "remove batch final corrected data shape: (6000, 200)\n",
      "DEBUG: Shape of corrected data: (6000, 200)\n",
      "start remove_batch_effects\n",
      "Shape of data: (6000, 200)\n",
      "Shape of beta:  (6000, 4)\n",
      "Beta_reduced contains 0 Nan values\n",
      "Shape of corrected data after correction: (6000, 200)\n",
      "index is Index(['prt1', 'prt10', 'prt100', 'prt1000', 'prt1001', 'prt1002', 'prt1003',\n",
      "       'prt1004', 'prt1005', 'prt1006',\n",
      "       ...\n",
      "       'prt990', 'prt991', 'prt992', 'prt993', 'prt994', 'prt995', 'prt996',\n",
      "       'prt997', 'prt998', 'prt999'],\n",
      "      dtype='object', name='rowname', length=6000)\n",
      "Amount of index found in hash2feature: 6000/6000\n",
      "After renaming got this data_corrected:               s.5      s.11      s.15      s.16      s.21      s.26      s.27  \\\n",
      "rowname                                                                         \n",
      "prt1    -0.973296 -0.683428 -0.183456  1.543500  0.946903  0.622240 -0.896166   \n",
      "prt10   -1.575114  0.497089 -0.080518 -0.879288  0.189994 -3.942982  1.940815   \n",
      "prt100  -2.092824 -2.089567 -3.377816 -3.392215  0.745987  3.445352 -1.365118   \n",
      "prt1000 -7.276162 -2.213098  2.024077 -0.137627  4.983540  1.797908  1.304355   \n",
      "prt1001  0.397684 -1.959629 -0.631975 -0.801404 -2.092044  0.529833 -2.434157   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "prt995   0.240572  1.057718 -0.841427  1.620456  0.064371 -0.273225  2.663931   \n",
      "prt996   1.806283  0.120392 -0.158953  2.051174 -0.289151 -1.029983  0.414688   \n",
      "prt997  -0.128518  0.415899 -0.399727 -0.997907 -1.461371 -0.155204  0.973389   \n",
      "prt998   2.895954  3.089059  2.767899  1.051961  1.201966  3.459065  0.939458   \n",
      "prt999   0.508656  2.176626 -2.797878  1.292577  1.114734 -2.793692  2.797550   \n",
      "\n",
      "             s.31      s.34      s.35  ...     s.573     s.575     s.577  \\\n",
      "rowname                                ...                                 \n",
      "prt1     0.446385  0.030274  1.379235  ... -1.051233  0.806537 -1.162550   \n",
      "prt10    0.924403  0.196739 -2.716337  ... -0.294912 -1.676179 -3.434782   \n",
      "prt100   1.844560 -0.662002 -0.172523  ...  2.097992  2.008906 -4.926183   \n",
      "prt1000  3.825021 -5.649563  3.120850  ...  2.900486 -7.381083 -6.338830   \n",
      "prt1001 -0.269092 -2.569790 -0.472238  ... -2.769046  0.039469 -0.835384   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "prt995   0.645702 -2.746151 -1.805051  ... -3.135315 -2.786262 -1.529777   \n",
      "prt996   0.571818  1.178323  0.545478  ... -1.347198 -1.789930  0.130189   \n",
      "prt997  -0.929025  1.382799 -2.399766  ... -2.661201 -0.623408 -1.018733   \n",
      "prt998   1.100092  0.387280  2.325593  ...  0.302881  0.459246  1.141146   \n",
      "prt999   0.658225 -0.933204  0.070541  ... -1.719953 -1.224058 -0.312507   \n",
      "\n",
      "            s.579     s.581     s.586     s.588     s.597     s.598     s.599  \n",
      "rowname                                                                        \n",
      "prt1    -0.079387  0.452027  0.442755 -2.561167  0.363469 -0.741896  0.837123  \n",
      "prt10   -1.536992 -0.342713 -1.347132 -3.580629 -2.549615 -1.613292  1.292186  \n",
      "prt100  -0.553232  1.150324  1.199130  0.357638 -5.818598 -2.632180 -4.353863  \n",
      "prt1000  0.448053 -1.827666  4.245576 -6.945392  1.735382  6.038086 -3.258902  \n",
      "prt1001 -2.610238 -0.173405 -1.042241 -0.328359 -0.067822 -1.402457 -0.943444  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "prt995  -2.400078 -1.859084 -0.688571  1.483730 -0.253464  0.118711 -0.689526  \n",
      "prt996   1.265181 -1.389327 -2.296423 -0.408467  1.435837 -1.320531 -2.834247  \n",
      "prt997   0.215296 -1.810986  0.368945 -0.739471 -2.388383 -3.240501 -2.249882  \n",
      "prt998  -0.824834  3.311478  0.416378  0.493649 -0.371539 -0.496144 -1.475674  \n",
      "prt999  -0.911852 -2.340841 -1.145037 -0.258597 -0.860508 -1.023536  1.890410  \n",
      "\n",
      "[6000 rows x 200 columns]\n",
      "remove batch final corrected data shape: (6000, 200)\n",
      "DEBUG: Shape of corrected data: (6000, 200)\n",
      "start remove_batch_effects\n",
      "Shape of data: (6000, 200)\n",
      "Shape of beta:  (6000, 4)\n",
      "Beta_reduced contains 0 Nan values\n",
      "Shape of corrected data after correction: (6000, 200)\n",
      "index is Index(['prt1', 'prt10', 'prt100', 'prt1000', 'prt1001', 'prt1002', 'prt1003',\n",
      "       'prt1004', 'prt1005', 'prt1006',\n",
      "       ...\n",
      "       'prt990', 'prt991', 'prt992', 'prt993', 'prt994', 'prt995', 'prt996',\n",
      "       'prt997', 'prt998', 'prt999'],\n",
      "      dtype='object', name='rowname', length=6000)\n",
      "Amount of index found in hash2feature: 6000/6000\n",
      "After renaming got this data_corrected:               s.1       s.2       s.6       s.7      s.10      s.13      s.14  \\\n",
      "rowname                                                                         \n",
      "prt1    -1.175712  1.903796  2.293759 -2.169912  1.245794  1.020908 -0.020188   \n",
      "prt10    0.679331  0.561517 -2.605598 -1.997427  0.310097 -0.943200 -2.314983   \n",
      "prt100   2.636939 -4.165481  1.704862 -2.820829  0.300654  0.234294  1.340320   \n",
      "prt1000  1.995147  1.829642 -1.515531 -1.888476  3.080022  3.108381  4.799387   \n",
      "prt1001  0.774584  0.309396 -0.859717  2.141536 -1.189449  2.074937 -0.275989   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "prt995   1.214082 -2.932638  3.011587  0.057000 -1.593626  1.858107 -2.279632   \n",
      "prt996   0.859202 -0.971182 -1.480004  1.854861  1.151115  1.062217  1.858251   \n",
      "prt997   0.212320  1.771765 -1.369378 -2.018703  0.122153  0.434560 -0.703330   \n",
      "prt998  -0.726022  5.128589 -1.123628 -0.546348  2.234173 -1.197699  2.352195   \n",
      "prt999   2.009618 -0.471475  1.626132 -1.890175  1.328044  2.810402  1.108476   \n",
      "\n",
      "             s.18      s.22      s.28  ...     s.549     s.550     s.556  \\\n",
      "rowname                                ...                                 \n",
      "prt1    -0.709773 -0.613288  2.151695  ... -1.233338  1.149937 -1.557250   \n",
      "prt10   -1.459527 -0.074188 -1.086218  ... -3.395477 -4.271454 -0.639329   \n",
      "prt100  -0.455720  4.924391  2.664988  ...  0.475414 -2.726569  2.643396   \n",
      "prt1000 -0.474071  0.573658 -6.149898  ... -1.696044 -2.095797  5.311144   \n",
      "prt1001  1.848575  0.403769 -0.228297  ... -2.464253 -1.326360 -1.457793   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "prt995   0.619895  1.573804 -2.522231  ... -1.236476 -2.834499 -0.695299   \n",
      "prt996  -1.422087 -0.123877 -0.075269  ... -1.703970 -0.386525 -0.060426   \n",
      "prt997   0.243023 -0.779240 -2.294692  ... -1.883222 -1.198562 -2.755269   \n",
      "prt998   0.883135  1.494303 -0.390564  ...  1.087666  0.283288 -0.669629   \n",
      "prt999  -2.361149 -0.358773  2.296280  ... -0.380698 -1.401965 -2.284796   \n",
      "\n",
      "            s.561     s.563     s.576     s.582     s.585     s.590     s.593  \n",
      "rowname                                                                        \n",
      "prt1     0.321368  0.281529  0.126803 -1.900413 -0.154319 -0.375088 -0.854295  \n",
      "prt10   -2.250827 -1.737220 -2.307997 -1.809197 -2.868030 -3.113735 -4.728301  \n",
      "prt100  -0.370663  4.309989  0.481724  0.142203 -1.899087  2.000417 -1.092689  \n",
      "prt1000 -7.479759  2.576236 -5.274298 -1.355101  0.609691  0.129916 -4.496878  \n",
      "prt1001 -0.697650 -0.814510 -1.073350 -1.053528 -0.981118 -1.381519 -1.851812  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "prt995  -0.611737  1.341276  0.022642  2.204553  0.083010 -0.891131  0.114111  \n",
      "prt996   1.818357 -0.631017 -4.823662 -1.754293 -2.275789 -2.093368 -1.724317  \n",
      "prt997  -0.599457 -1.567220 -1.834664 -4.923194  0.696053 -3.392850 -1.705482  \n",
      "prt998   1.505646  0.215001  1.147862  0.322910  0.326906  0.648338  0.874012  \n",
      "prt999   0.188144 -1.250118  3.007286 -3.561340 -1.336833 -0.093517 -0.900892  \n",
      "\n",
      "[6000 rows x 200 columns]\n",
      "remove batch final corrected data shape: (6000, 200)\n",
      "DEBUG: Shape of corrected data: (6000, 200)\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: All: include_correction\n",
    "### Corrects the individual data\n",
    "for clientWrapper in clientWrappers:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "\n",
    "    # remove the batch effects in own data and safe the results\n",
    "    client.remove_batch_effects(beta)\n",
    "    print(f\"DEBUG: Shape of corrected data: {client.data_corrected.shape}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time\n",
    "\n",
    "    # As this is a simulation we don't save the corrected data to csv, instead\n",
    "    # we save it as a variable to the clientwrapper\n",
    "    clientWrapper.data_corrected = client.data_corrected\n",
    "    # client.data_corrected.to_csv(os.path.join(os.getcwd(), \"mnt\", \"output\", \"only_batch_corrected_data.csv\"),\n",
    "    #                                 sep=self.load(\"separator\"))\n",
    "    # client.data_corrected_and_raw.to_csv(os.path.join(os.getcwd(), \"mnt\", \"output\", \"all_data.csv\"),\n",
    "    #                              sep=self.load(\"separator\"))\n",
    "    # with open(os.path.join(os.getcwd(), \"mnt\", \"output\", \"report.txt\"), \"w\") as f:\n",
    "    #     f.write(client.report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time tracker for coordinator, ms: 308.59\n",
      "Time tracker for lab1, ms: 635.72\n",
      "Time tracker for lab2, ms: 679.99\n",
      "Time tracker for lab3, ms: 689.31\n"
     ]
    }
   ],
   "source": [
    "# print the time tracker for the coordinator\n",
    "print(f\"Time tracker for coordinator, ms: {round(time_tracker['Coordinator']*1000, 2)}\")\n",
    "\n",
    "# print the time tracker for the clients\n",
    "for clientWrapper in clientWrappers:\n",
    "    print(f\"Time tracker for {clientWrapper.id}, ms: {round(time_tracker[clientWrapper.id]*1000, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "###                                  INFO                                  ###\n",
    "###                            SIMULATION IS DONE                          ###\n",
    "### The simulation is done. The corrected data is saved in the             ###\n",
    "### clientWrapper instances. Now we analyse the data by comparing to the   ###\n",
    "### calculated centralized corrected data.                                 ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_df, intersect_features = _concat_federated_results(clientWrappers, samples_in_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE RESULTS ###\n",
    "# Microarray data\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"microarray\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# # Proteomics data\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"proteomics\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Microbiome data\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"microbiome\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# Simulation data\n",
    "federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"simulated\", \"balanced\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"simulated\", \"mild_imbalanced\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n",
    "# federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", \"simulated\", \"strong_imbalanced\", \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________Analysing: microarray_________________________\n",
      "Max difference: 8.171241461241152e-14\n",
      "Mean difference: 4.357000735613189e-15\n",
      "Max diff at position: GSM1701030\n",
      "Max difference in intersect: 8.171241461241152e-14\n",
      "Mean difference in intersect: 4.515621419262236e-15\n",
      "Max diff at position in intersect: GSM1701030\n"
     ]
    }
   ],
   "source": [
    "### Concat the federated data and read in the centralized data ###\n",
    "central_df_path = os.path.join(os.path.dirname(base_dir), \"after\", \"central_corrected_UNION.tsv\")\n",
    "central_df = pd.read_csv(central_df_path, sep=\"\\t\", index_col=0)\n",
    "_compare_central_federated_dfs(\"microarray\", central_df, federated_df, intersect_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________Analysing: proteomic data_________________________\n",
      "Max difference: 1.9184653865522705e-13\n",
      "Mean difference: 3.1758692034102934e-14\n",
      "Max diff at position: Clinspect_E_coli_A_S42_Slot1-21_1_8670\n",
      "Max difference in intersect: 1.1368683772161603e-13\n",
      "Mean difference in intersect: 3.338152465379177e-14\n",
      "Max diff at position in intersect: Clinspect_E_coli_A_S42_Slot1-21_1_8670\n"
     ]
    }
   ],
   "source": [
    "### Concat the federated data and read in the centralized data ###\n",
    "central_df_path = os.path.join(os.path.dirname(os.path.dirname(base_dir)), \"proteomics\", \"after\", \"intensities_log_Rcorrected_UNION.tsv\")\n",
    "central_df = pd.read_csv(central_df_path, sep=\"\\t\", index_col=0)\n",
    "_compare_central_federated_dfs(\"proteomic data\", central_df, federated_df, intersect_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
