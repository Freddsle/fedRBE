{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "import os\n",
    "from typing import List, Tuple, Union, Dict\n",
    "from classes.client import Client\n",
    "from classes.coordinator_utils import select_common_features_variables, \\\n",
    "    compute_beta, create_beta_mask\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "### Just run this, these functions are needed in various places ###\n",
    "\n",
    "### Define the client class ###\n",
    "class ClientWrapper:\n",
    "    \"\"\"\n",
    "    Holds all information necessary for the simulation run to work.\n",
    "    Defines the input data of the client, it's name and if it should be\n",
    "    considered as the coordinator\n",
    "    \"\"\"\n",
    "    def __init__(self, id: str, input_folder: str, coordinator: bool = False):\n",
    "        self.id = id\n",
    "        self.input_folder = input_folder\n",
    "        self.is_coordinator = coordinator\n",
    "        self.client_class: Client # initiated later\n",
    "        self.data_corrected: pd.DataFrame # initiated later\n",
    "\n",
    "def _check_consistency_clientwrappers(clientWrappers: List[ClientWrapper]) -> None:\n",
    "    \"\"\"\n",
    "    Checks for a list of clients if they were created correctly\n",
    "    Raises a ValueError in case of inconsistencies\n",
    "    Checks:\n",
    "        1. If exactly one coordinator exists\n",
    "    \"\"\"\n",
    "    coord = False\n",
    "    for clientWrapper in clientWrappers:\n",
    "        if coord and clientWrapper.is_coordinator:\n",
    "            raise ValueError(\"More than one coordinator was defined, please check \"+\\\n",
    "                            \"the code defining the clients\")\n",
    "        if clientWrapper.is_coordinator:\n",
    "            coord = True\n",
    "    if not coord:\n",
    "        raise ValueError(\"No client instance is a coordinator, please designate \"+\\\n",
    "                        \"any client as a coordinator\")\n",
    "\n",
    "def _compare_central_federated_dfs(name:str,\n",
    "                                   central_df: pd.DataFrame,\n",
    "                                   federated_df: pd.DataFrame,\n",
    "                                   intersection_features: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Compares two dataframes for equality. First checks that index and columns\n",
    "    are the same, then analyses the element wise differences.\n",
    "    See the analyse_diff_df function for more details on the difference analysis.\n",
    "    If both dataframes contain a NaN value at the same position, this is considered\n",
    "    as equal (0 as difference).\n",
    "    Args:\n",
    "        name: Name used for printing\n",
    "        central_df: The central dataframe\n",
    "        federated_df: The federated dataframe\n",
    "        intersection_features: The features that are common to both dataframes\n",
    "    \"\"\"\n",
    "    central_df = central_df.sort_index(axis=0).sort_index(axis=1)\n",
    "    federated_df = federated_df.sort_index(axis=0).sort_index(axis=1)\n",
    "    print(f\"_________________________Analysing: {name}_________________________\")\n",
    "    ### compare columns and index ###\n",
    "    failed = False\n",
    "    if not central_df.columns.equals(federated_df.columns):\n",
    "        print(f\"Columns do not match for central_df and federated_df\")\n",
    "        union_cols = central_df.columns.union(federated_df.columns)\n",
    "        intercept_cols = central_df.columns.intersection(federated_df.columns)\n",
    "        print(f\"Union-Intercept of columns: {union_cols.difference(intercept_cols)}\")\n",
    "        failed = True\n",
    "    if not central_df.index.equals(federated_df.index):\n",
    "        print(f\"Rows do not match for central_df and federated_df\")\n",
    "        union_rows = central_df.index.union(federated_df.index)\n",
    "        intercept_rows = central_df.index.intersection(federated_df.index)\n",
    "        print(f\"Union-Intercept of rows: {union_rows.difference(intercept_rows)}\")\n",
    "        failed = True\n",
    "    if failed:\n",
    "        print(f\"_________________________FAILED: {name}_________________________\")\n",
    "\n",
    "    df_diff = (central_df - federated_df).abs()\n",
    "    print(f\"Max difference: {df_diff.max().max()}\")\n",
    "    print(f\"Mean difference: {df_diff.mean().mean()}\")\n",
    "    print(f\"Max diff at position: {df_diff.idxmax().idxmax()}\")\n",
    "\n",
    "    df_diff_intersect = df_diff.loc[intersection_features]\n",
    "    print(f\"Max difference in intersect: {df_diff_intersect.max().max()}\")\n",
    "    print(f\"Mean difference in intersect: {df_diff_intersect.mean().mean()}\")\n",
    "    print(f\"Max diff at position in intersect: {df_diff_intersect.idxmax().idxmax()}\")\n",
    "\n",
    "def _concat_federated_results(clientWrappers: List[ClientWrapper],\n",
    "                              samples_in_columns=True) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Concatenates the results of the federated clients into one dataframe\n",
    "    Also checks which features are common to all clients\n",
    "    and returns them\n",
    "    Args:\n",
    "        clientWrappers: List of ClientWrapper instances, containing the data\n",
    "            in the data_corrected attribute\n",
    "        samples_in_columns: If True, the samples are in the columns, if False\n",
    "            they are in the rows. For expression files this is true.\n",
    "            This decides how to aggregate the dataframes\n",
    "    Returns:\n",
    "        merged_df: The merged dataframe containing the data of all clients\n",
    "        intersection_features: The features that are common to all clients\n",
    "    \"\"\"\n",
    "    merged_df = None\n",
    "    intersection_features = set()\n",
    "    for clientWrapper in clientWrappers:\n",
    "        # get the data in the correct format\n",
    "        if not hasattr(clientWrapper, \"data_corrected\") or \\\n",
    "            clientWrapper.data_corrected is None:\n",
    "            raise ValueError(\"No data was found in the clientWrappers\")\n",
    "        corrected_data = clientWrapper.data_corrected\n",
    "        if not samples_in_columns:\n",
    "            corrected_data = corrected_data.T\n",
    "\n",
    "        cleaned_corrected_features = set(corrected_data.dropna().index)\n",
    "        # initialize the merged_df\n",
    "        if merged_df is None:\n",
    "            merged_df = corrected_data\n",
    "            intersection_features = cleaned_corrected_features\n",
    "            continue\n",
    "\n",
    "        # merge the data\n",
    "        merged_df = pd.concat([merged_df, corrected_data], axis=1)\n",
    "        intersection_features = intersection_features.intersection(cleaned_corrected_features)\n",
    "\n",
    "    # final check\n",
    "    if merged_df is None:\n",
    "        raise ValueError(\"No data was found in the clientWrappers\")\n",
    "    # reverse the Transpose if necessary\n",
    "    if not samples_in_columns:\n",
    "        merged_df = merged_df.T\n",
    "    return merged_df, list(intersection_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This part defines the data used. A ClientWrapper class is used to      ###\n",
    "### describe all cohorts.                                                  ###\n",
    "### Comment in the wanted data or add a new block for new data. No other   ###\n",
    "### changes are necessary.                                                 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Microarray Data ####################\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# datafolder = \"microarray\"\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"microarray\", \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'GSE38666',  # Client 1 (Coordinator)\n",
    "#     'GSE14407',  # Client 2\n",
    "#     'GSE6008',   # Client 3\n",
    "#     'GSE40595',  # Client 4\n",
    "#     'GSE26712',  # Client 5\n",
    "#     'GSE69428',  # Client 6\n",
    "# ]\n",
    "# output_name = \"microarray\"\n",
    "# central_filename = \"central_corrected_UNION.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Proteomics Data ####################\n",
    "\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# datafolder = \"proteomics\"\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", datafolder, \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'lab_A',  # Client 1 (Coordinator)\n",
    "#     'lab_B',  # Client 2\n",
    "#     'lab_C',  # Client 3\n",
    "#     'lab_D',  # Client 4\n",
    "#     'lab_E',  # Client 5\n",
    "# ]\n",
    "# output_name = \"proteomics\"\n",
    "# central_filename = \"intensities_log_Rcorrected_UNION.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Proteomics Multibatch Data ####################\n",
    "\n",
    "# First define the basefolder where all files are located\n",
    "base_dir = os.path.join(\"..\")\n",
    "# Go back to the git repos root dir\n",
    "datafolder = \"proteomics_multibatch\"\n",
    "base_dir = os.path.join(base_dir, \"evaluation_data\", datafolder, \"before\")\n",
    "\n",
    "# List of cohort names\n",
    "cohort_names = [\n",
    "    'center1', # Client 1 (Coordinator)\n",
    "    'center2', # Client 2\n",
    "    'center3' # Client 3\n",
    "]\n",
    "output_name = \"proteomics_multibatch\"\n",
    "central_filename = \"intensities_log_Rcorrected_UNION.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# Microbiome Data ####################\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "# datafolder = \"microbiome\"\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", datafolder, \"before\")\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'PRJEB27928',  # Client 1 (Coordinator)\n",
    "#     'PRJEB6070',   # Client 2\n",
    "#     'PRJNA429097', # Client 3\n",
    "#     'PRJEB10878',  # Client 4\n",
    "#     'PRJNA731589', # Client 5\n",
    "# ]\n",
    "# output_name = \"microbiome\"\n",
    "# central_filename = \"normalized_logmin_counts_5centers_Rcorrected.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################### Simulation Data ###################\n",
    "\n",
    "# # First define the basefolder where all files are located\n",
    "# base_dir = os.path.join(\"..\")\n",
    "# # Go back to the git repos root dir\n",
    "\n",
    "# # There are three different setups, activate the wanted one here accordingly, deactivate the others:\n",
    "# # BALANCED\n",
    "# datafolder = os.path.join(\"simulated\", \"balanced\")\n",
    "# base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"balanced\", \"before\")\n",
    "# output_name = \"simulated_balanced\"\n",
    "# # # MILD IMBALANCED\n",
    "# # datafolder = os.path.join(\"simulated\", \"mild_imbalanced\")\n",
    "# # base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"mild_imbalanced\", \"before\")\n",
    "# # output_name = \"simulated_mild_imbalanced\"\n",
    "# # # STRONG IMBALANCED\n",
    "# # datafolder = os.path.join(\"simulated\", \"strong_imbalanced\")\n",
    "# # base_dir = os.path.join(base_dir, \"evaluation_data\", \"simulated\", \"strong_imbalanced\", \"before\")\n",
    "# # output_name = \"simulated_strong_imbalanced\"\n",
    "\n",
    "# # List of cohort names\n",
    "# cohort_names = [\n",
    "#     'lab1',  # Client 1 (Coordinator)\n",
    "#     'lab2',  # Client 2\n",
    "#     'lab3',  # Client 3\n",
    "# ]\n",
    "# central_filename = \"intensities_R_corrected.tsv\" # is the same for all simulated setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize clientWrappers list\n",
    "clientWrappers: List[ClientWrapper] = []\n",
    "\n",
    "# Iterate over cohort names and create ClientWrapper instances\n",
    "for i, cohortname in enumerate(cohort_names):\n",
    "    clientWrappers.append(ClientWrapper(\n",
    "        id=cohortname,\n",
    "        input_folder=os.path.join(base_dir, cohortname),\n",
    "        coordinator=(i == 0)  # Set the first client as coordinator\n",
    "    ))\n",
    "\n",
    "# Double check that we only have one coordinator\n",
    "_check_consistency_clientwrappers(clientWrappers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure time for all clients\n",
    "time_tracker = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###                                  INFO                                  ###\n",
    "### The following code blocks run the simulation. They are divided into    ###\n",
    "### multiple logical blocks to ease the use                                ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'batch_col': 'batch', 'covariates': ['Pyr'], 'data_filename': 'intensities_log_UNION.tsv', 'design_filename': 'design.tsv', 'design_separator': '\\t', 'expression_file_flag': True, 'index_col': 'rowname', 'min_samples': 0, 'normalizationMethod': None, 'position': 0, 'separator': '\\t', 'smpc': False}}\n",
      "Opening dataset ../evaluation_data/proteomics_multibatch/before/center1/intensities_log_UNION.tsv\n",
      "Shape of rawdata(expr_file): (2910, 47)\n",
      "finished loading data, shape of data: (2910, 47), num_features: 2910, num_samples: 47\n",
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'batch_col': 'batch', 'covariates': ['Pyr'], 'data_filename': 'intensities_log_UNION.tsv', 'design_filename': 'design.tsv', 'design_separator': '\\t', 'expression_file_flag': True, 'index_col': 'rowname', 'min_samples': 0, 'normalizationMethod': None, 'position': 1, 'separator': '\\t', 'smpc': False}}\n",
      "Opening dataset ../evaluation_data/proteomics_multibatch/before/center2/intensities_log_UNION.tsv\n",
      "Shape of rawdata(expr_file): (2900, 47)\n",
      "finished loading data, shape of data: (2900, 47), num_features: 2900, num_samples: 47\n",
      "Got the following config:\n",
      "{'flimmaBatchCorrection': {'batch_col': 'batch', 'covariates': ['Pyr'], 'data_filename': 'intensities_log_UNION.tsv', 'design_filename': 'design.tsv', 'design_separator': '\\t', 'expression_file_flag': True, 'index_col': 'rowname', 'min_samples': 0, 'normalizationMethod': None, 'position': 2, 'separator': '\\t', 'smpc': False}}\n",
      "Opening dataset ../evaluation_data/proteomics_multibatch/before/center3/intensities_log_UNION.tsv\n",
      "Shape of rawdata(expr_file): (2363, 24)\n",
      "finished loading data, shape of data: (2363, 24), num_features: 2363, num_samples: 24\n",
      "INFO: Each feature must at least have 7 samples in a batch to be considered for batch effect correction. Otherwise, privacy cannot be guaranteed.\n",
      "INFO: Detecting unsecure features...\n",
      "INFO: Each feature must at least have 7 samples in a batch to be considered for batch effect correction. Otherwise, privacy cannot be guaranteed.\n",
      "INFO: Detecting unsecure features...\n",
      "INFO: Each feature must at least have 7 samples in a batch to be considered for batch effect correction. Otherwise, privacy cannot be guaranteed.\n",
      "INFO: Detecting unsecure features...\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: all: initial ###\n",
    "### Initial reading of the input folder\n",
    "\n",
    "send_batch_labels_covariates = list()\n",
    "send_feature_information = list()\n",
    "for clientWrapper in clientWrappers:\n",
    "    # define the client class\n",
    "    cohort_name = clientWrapper.id\n",
    "    client = Client()\n",
    "    client.config_based_init(clientname = cohort_name,\n",
    "                             input_folder = clientWrapper.input_folder,\n",
    "                             use_hashing = True)\n",
    "    assert isinstance(client.hash2feature, dict)\n",
    "    assert isinstance(client.hash2variable, dict)\n",
    "    clientWrapper.client_class = client\n",
    "    # send the batch labels and covariates\n",
    "    send_batch_labels_covariates.append((client.batch_labels, client.hash2variable.keys()))\n",
    "\n",
    "# receive the batch labels and covariates\n",
    "# intersect covariates, union labels\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "        global_variables_hashed = set()\n",
    "        global_batch_labels = list()\n",
    "        for labels, variables in send_batch_labels_covariates:\n",
    "            # intersect the variables\n",
    "            if len(global_variables_hashed) == 0:\n",
    "                global_variables_hashed = set(variables)\n",
    "            else:\n",
    "                global_variables_hashed = global_variables_hashed.intersection(set(variables))\n",
    "            # extend the batch_labels\n",
    "            global_batch_labels.extend(labels)\n",
    "        # ensure the batch_labels are unique\n",
    "        if len(global_batch_labels) != len(set(global_batch_labels)):\n",
    "            raise ValueError(\"Batch labels are not unique\")\n",
    "        num_batches = len(global_batch_labels)\n",
    "\n",
    "# get the relevant and privacy preserving features\n",
    "for clientWrapper in clientWrappers:\n",
    "    cohort_name = clientWrapper.id\n",
    "    client = clientWrapper.client_class\n",
    "    min_samples = max(num_batches+len(global_variables_hashed)+1, client.min_samples)\n",
    "    batch_feature_presence_info: Dict[str, List[str]] = client.get_batch_feature_presence_info(min_samples=min_samples)\n",
    "    send_feature_information.append((cohort_name,\n",
    "                                client.position,\n",
    "                                batch_feature_presence_info))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using given specific client order: ['center1', 'center2', 'center3']\n",
      "INFO: Cohorts order: ['center1|lab_A', 'center1|lab_B', 'center2|lab_C', 'center2|lab_D', 'center3|lab_E']\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: Coordinator: global_feature_selection ###\n",
    "### Aggregate the features and variables\n",
    "\n",
    "# obtain and safe common genes and indices of design matrix\n",
    "# wait for each client to send the list of genes they have\n",
    "# also memo the feature presence matrix and feature_to_cohorts\n",
    "\n",
    "broadcast_features_variables = tuple()\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "\n",
    "        time_tracker[\"Coordinator\"] = time.time()\n",
    "\n",
    "        global_feature_names, feature_presence_matrix, cohorts_order = \\\n",
    "            select_common_features_variables(\n",
    "                feature_batch_info=send_feature_information,\n",
    "                min_clients=3,\n",
    "                default_order=cohort_names\n",
    "            )\n",
    "        # memo the feature presence matrix and feature_to_cohorts\n",
    "        broadcast_features_variables = global_feature_names, cohorts_order\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] = end_time - time_tracker[\"Coordinator\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Client center1: Inputs validated.\n",
      "Number of features available in this client: 2910\n",
      "Number of features given globally: 2694\n",
      "Number of features only available on this client: 221\n",
      "Number of features available in other clients but not this client: 5\n",
      "Adding 5 extra global features\n",
      "INFO: Client center2: Inputs validated.\n",
      "Number of features available in this client: 2900\n",
      "Number of features given globally: 2694\n",
      "Number of features only available on this client: 207\n",
      "Number of features available in other clients but not this client: 1\n",
      "Adding 1 extra global features\n",
      "INFO: Client center3: Inputs validated.\n",
      "Number of features available in this client: 2363\n",
      "Number of features given globally: 2694\n",
      "Number of features only available on this client: 21\n",
      "Number of features available in other clients but not this client: 352\n",
      "Adding 352 extra global features\n",
      "INFO: Client center3 has only one batch\n",
      "INFO: Client center3 is the reference batch\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: All: validate ###\n",
    "### Expand data to fullfill the global format. Also performs consistency checks\n",
    "for clientWrapper in clientWrappers:\n",
    "\n",
    "    time_tracker[clientWrapper.id] = time.time()\n",
    "\n",
    "    global_feauture_names_hashed, cohorts_order = \\\n",
    "        broadcast_features_variables\n",
    "    client = clientWrapper.client_class\n",
    "    client.validate_inputs(global_variables_hashed)\n",
    "    client.set_data(global_feauture_names_hashed)\n",
    "\n",
    "    err = client.create_design(cohorts_order)\n",
    "    if err:\n",
    "        raise ValueError(err)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] = end_time - time_tracker[clientWrapper.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of global mask: (2694, 6)\n",
      "Head of mask: [[False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]]\n"
     ]
    }
   ],
   "source": [
    "### Simulatuion: Coordinator: create design mask based on feature presence matrix ###\n",
    "### Create the mask for the design matrix based on the feature presence matrix\n",
    "### that will be used for the beta computation\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "        start_time = time.time()\n",
    "        client = clientWrapper.client_class\n",
    "\n",
    "        n=len(client.feature_names)\n",
    "        k=client.design.shape[1]\n",
    "\n",
    "        global_mask = create_beta_mask(feature_presence_matrix, n, k)\n",
    "        print(f\"Shape of global mask: {global_mask.shape}\")\n",
    "        print(f\"Head of mask: {global_mask[:5]}\")\n",
    "        # memo the global mask\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: All: prepare for compute_XtX_XtY ###\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "    client.sample_names = client.design.index.values\n",
    "\n",
    "    # Error check if the design index and the data index are the same\n",
    "    # we check by comparing the sorted indexes\n",
    "    client._check_consistency_designfile()\n",
    "\n",
    "    # Extract only relevant (the global) features and samples\n",
    "    client.data = client.data.loc[client.feature_names, client.sample_names]\n",
    "    client.n_samples = len(client.sample_names)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: All: compute_XtX_XtY ###\n",
    "### Compute XtX and XtY and share it\n",
    "send_XtX_XtY_list: List[List[np.ndarray]] = list()\n",
    "for clientWrapper in clientWrappers:\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "\n",
    "    # compute XtX and XtY\n",
    "    XtX, XtY, err = client.compute_XtX_XtY()\n",
    "    if err != \"\":\n",
    "        raise ValueError(err)\n",
    "\n",
    "    # send XtX and XtY\n",
    "    send_XtX_XtY_list.append([XtX, XtY])\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION: Coordinator: compute_beta\n",
    "### Compute the beta values and broadcast them to the others\n",
    "broadcast_betas = None # np.ndarray of shape num_features x design_columns\n",
    "\n",
    "for clientWrapper in clientWrappers:\n",
    "    if clientWrapper.is_coordinator:\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        client = clientWrapper.client_class\n",
    "        beta = compute_beta(XtX_XtY_list=send_XtX_XtY_list,\n",
    "                            n=len(client.feature_names),\n",
    "                            k=client.design.shape[1],\n",
    "                            global_mask=global_mask)\n",
    "\n",
    "        # send beta to clients so they can correct their data\n",
    "        broadcast_betas = beta\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_tracker[\"Coordinator\"] += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Removing the batch effects with the trained betas\n",
      "INFO: Shape of corrected data after correction: (2689, 47)\n",
      "DEBUG: Shape of corrected data: (2689, 47)\n",
      "INFO: Removing the batch effects with the trained betas\n",
      "INFO: Shape of corrected data after correction: (2693, 47)\n",
      "DEBUG: Shape of corrected data: (2693, 47)\n",
      "INFO: Removing the batch effects with the trained betas\n",
      "INFO: Shape of corrected data after correction: (2342, 24)\n",
      "DEBUG: Shape of corrected data: (2342, 24)\n"
     ]
    }
   ],
   "source": [
    "### SIMULATION: All: include_correction\n",
    "### Corrects the individual data\n",
    "for clientWrapper in clientWrappers:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = clientWrapper.client_class\n",
    "\n",
    "    # remove the batch effects in own data and safe the results\n",
    "    client.remove_batch_effects(beta)\n",
    "    print(f\"DEBUG: Shape of corrected data: {client.data_corrected.shape}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_tracker[clientWrapper.id] += end_time - start_time\n",
    "\n",
    "    # As this is a simulation we don't save the corrected data to csv, instead\n",
    "    # we save it as a variable to the clientwrapper\n",
    "    clientWrapper.data_corrected = client.data_corrected\n",
    "    # client.data_corrected.to_csv(os.path.join(os.getcwd(), \"mnt\", \"output\", \"only_batch_corrected_data.csv\"),\n",
    "    #                                 sep=self.load(\"separator\"))\n",
    "    # client.data_corrected_and_raw.to_csv(os.path.join(os.getcwd(), \"mnt\", \"output\", \"all_data.csv\"),\n",
    "    #                              sep=self.load(\"separator\"))\n",
    "    # with open(os.path.join(os.getcwd(), \"mnt\", \"output\", \"report.txt\"), \"w\") as f:\n",
    "    #     f.write(client.report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time tracker for coordinator, ms: 180.17\n",
      "Time tracker for center1, ms: 282.26\n",
      "Time tracker for center2, ms: 274.74\n",
      "Time tracker for center3, ms: 245.18\n"
     ]
    }
   ],
   "source": [
    "# print the time tracker for the coordinator\n",
    "print(f\"Time tracker for coordinator, ms: {round(time_tracker['Coordinator']*1000, 2)}\")\n",
    "\n",
    "# print the time tracker for the clients\n",
    "for clientWrapper in clientWrappers:\n",
    "    print(f\"Time tracker for {clientWrapper.id}, ms: {round(time_tracker[clientWrapper.id]*1000, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###                                  INFO                                  ###\n",
    "###                            SIMULATION IS DONE                          ###\n",
    "### The simulation is done. The corrected data is saved in the             ###\n",
    "### clientWrapper instances. Now we analyse the data by comparing to the   ###\n",
    "### calculated centralized corrected data.                                 ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_df, intersect_features = _concat_federated_results(clientWrappers, samples_in_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE RESULTS ###\n",
    "#federated_df.to_csv(os.path.join(\"..\", \"evaluation_data\", datafolder, \"after\", \"FedSim_corrected_data.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________Analysing: proteomics_multibatch_________________________\n",
      "Rows do not match for central_df and federated_df\n",
      "Union-Intercept of rows: Index(['A5A607;Q2EES0', 'A5A613', 'P02920', 'P02931;P02932;P21420;P77747',\n",
      "       'P02931;P21420', 'P03835', 'P03960', 'P05707', 'P06609', 'P06965',\n",
      "       ...\n",
      "       'Q46904', 'Q46906', 'Q46938', 'Q47141', 'Q47149', 'Q47155', 'Q47158',\n",
      "       'Q47319', 'Q47536', 'Q47689'],\n",
      "      dtype='object', name='rowname', length=340)\n",
      "_________________________FAILED: proteomics_multibatch_________________________\n",
      "Max difference: 1.5631940186722204e-13\n",
      "Mean difference: 3.235154577009516e-14\n",
      "Max diff at position: BBM_673_P283_01_VEB_018_R2\n",
      "Max difference in intersect: 1.1723955140041653e-13\n",
      "Mean difference in intersect: 3.338969194626464e-14\n",
      "Max diff at position in intersect: CVT09_QC4_LabE_X026\n"
     ]
    }
   ],
   "source": [
    "### Concat the federated data and read in the centralized data ###\n",
    "base_dir = os.path.join(\"..\", \"evaluation_data\")\n",
    "central_df_path = os.path.join(base_dir, datafolder, \"after\", central_filename)\n",
    "central_df = pd.read_csv(central_df_path, sep=\"\\t\", index_col=0)\n",
    "_compare_central_federated_dfs(output_name, central_df, federated_df, intersect_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-omRVbgZA-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
